{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import math\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "% matplotlib inline\n",
    "\n",
    "# helper functions\n",
    "def normalize_column(col):\n",
    "    col = col.values.reshape(-1, 1).astype(\"float64\")\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    col_scaled = min_max_scaler.fit_transform(col)\n",
    "    return col_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portfolio Dataset\n",
    "portfolio is the smallest dataset and contains all offer information. We mainly preprocess two columns:\n",
    "- offer_type: there are three offer types, that is, bogo, informational, discount. We need process them into numeric categrory such as 1, 2, 3. \n",
    "- channels: a list of channel categories such as web, email, mobile, social. We could break it out and use one hot encoding representing this information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_portfolio(filepath=\"data/portfolio.json\"):\n",
    "    # read in the json files\n",
    "    portfolio = pd.read_json(filepath, orient='records', lines=True)\n",
    "\n",
    "    # numeric offer types\n",
    "    portfolio = pd.concat([\n",
    "        portfolio,\n",
    "        pd.get_dummies(portfolio[\"offer_type\"], prefix=\"offer_type\")\n",
    "    ], axis=1).drop([\"offer_type\"], axis=1)\n",
    "    \n",
    "    # break out channels column\n",
    "    portfolio[\"channel_email\"] = portfolio[\"channels\"].apply(lambda x: int(\"email\" in x))\n",
    "    portfolio[\"channel_web\"] = portfolio[\"channels\"].apply(lambda x: int(\"web\" in x))\n",
    "    portfolio[\"channel_mobile\"] = portfolio[\"channels\"].apply(lambda x: int(\"mobile\" in x))\n",
    "    portfolio[\"channel_social\"] = portfolio[\"channels\"].apply(lambda x: int(\"social\" in x))\n",
    "    \n",
    "    # two ratio features\n",
    "    portfolio[\"reward_difficulty\"] = portfolio[\"reward\"] / portfolio[\"difficulty\"]\n",
    "    portfolio[\"reward_difficulty\"] = portfolio[\"reward_difficulty\"].fillna(0)\n",
    "    portfolio[\"difficulty_duration\"] = portfolio[\"difficulty\"] / portfolio[\"duration\"]\n",
    "    \n",
    "    # normalize columns\n",
    "    portfolio[\"difficulty\"] = normalize_column(portfolio[\"difficulty\"])\n",
    "    portfolio[\"duration\"] = normalize_column(portfolio[\"duration\"])\n",
    "    portfolio[\"reward\"] = normalize_column(portfolio[\"reward\"])\n",
    "    \n",
    "    # drop channels\n",
    "    return portfolio.drop([\"channels\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>difficulty</th>\n",
       "      <th>duration</th>\n",
       "      <th>id</th>\n",
       "      <th>reward</th>\n",
       "      <th>offer_type_bogo</th>\n",
       "      <th>offer_type_discount</th>\n",
       "      <th>offer_type_informational</th>\n",
       "      <th>channel_email</th>\n",
       "      <th>channel_web</th>\n",
       "      <th>channel_mobile</th>\n",
       "      <th>channel_social</th>\n",
       "      <th>reward_difficulty</th>\n",
       "      <th>difficulty_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>ae264e3637204a6fb9bb56bc8210ddfd</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>4d5c57ea9a6940dd891ad53e9dbe8da0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>3f207df678b143eea3cee63160fa8bed</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>9b98b8c7a33c4b65b9aebfe6a799e6d9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0b1e1539f2cc45b7b9fa7c272da2e1d7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   difficulty  duration                                id  reward  \\\n",
       "0        0.50  0.571429  ae264e3637204a6fb9bb56bc8210ddfd     1.0   \n",
       "1        0.50  0.285714  4d5c57ea9a6940dd891ad53e9dbe8da0     1.0   \n",
       "2        0.00  0.142857  3f207df678b143eea3cee63160fa8bed     0.0   \n",
       "3        0.25  0.571429  9b98b8c7a33c4b65b9aebfe6a799e6d9     0.5   \n",
       "4        1.00  1.000000  0b1e1539f2cc45b7b9fa7c272da2e1d7     0.5   \n",
       "\n",
       "   offer_type_bogo  offer_type_discount  offer_type_informational  \\\n",
       "0                1                    0                         0   \n",
       "1                1                    0                         0   \n",
       "2                0                    0                         1   \n",
       "3                1                    0                         0   \n",
       "4                0                    1                         0   \n",
       "\n",
       "   channel_email  channel_web  channel_mobile  channel_social  \\\n",
       "0              1            0               1               1   \n",
       "1              1            1               1               1   \n",
       "2              1            1               1               0   \n",
       "3              1            1               1               0   \n",
       "4              1            1               0               0   \n",
       "\n",
       "   reward_difficulty  difficulty_duration  \n",
       "0               1.00             1.428571  \n",
       "1               1.00             2.000000  \n",
       "2               0.00             0.000000  \n",
       "3               1.00             0.714286  \n",
       "4               0.25             2.000000  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "portfolio_df = process_portfolio()\n",
    "portfolio_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After preprocess, we could see some property of this dataset. \n",
    "1. The most of offer has rewards. 4 discounts and 4 BOGO. \n",
    "2. Reward of BOGO are higher than that of discount.\n",
    "3. Email is main spreading channel while social is the least. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profile Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_profile(filepath='data/profile.json'):\n",
    "    # read in the json files\n",
    "    profile = pd.read_json(filepath, orient='records', lines=True)\n",
    "    \n",
    "    # fillin income nan with median\n",
    "    profile = profile.fillna(\n",
    "        {\"income\": profile.income.dropna().median()})\n",
    "    \n",
    "    # fillin age nan with median\n",
    "    profile.age = profile.age.apply(lambda x: None if x == 118 else x)\n",
    "    profile = profile.fillna({\"age\": profile.age.dropna().median()})\n",
    "    \n",
    "    # one hot encoding for gender\n",
    "    profile = pd.concat([profile, \n",
    "                         pd.get_dummies(profile['gender'], prefix='gender', dummy_na=True)\n",
    "                        ], axis=1).drop([\"gender\"], axis=1)\n",
    "    \n",
    "    # convert date to unix timestamp\n",
    "    profile.became_member_on = pd.to_datetime(\n",
    "        profile.became_member_on, format='%Y%m%d').astype(np.int64) // 10**9\n",
    "    \n",
    "    # normalize\n",
    "    profile[\"age\"] = normalize_column(profile[\"age\"])\n",
    "    profile[\"became_member_on\"] = normalize_column(profile[\"became_member_on\"])\n",
    "    profile[\"income\"] = normalize_column(profile[\"income\"])\n",
    "    \n",
    "    return profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>became_member_on</th>\n",
       "      <th>id</th>\n",
       "      <th>income</th>\n",
       "      <th>gender_F</th>\n",
       "      <th>gender_M</th>\n",
       "      <th>gender_O</th>\n",
       "      <th>gender_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.445783</td>\n",
       "      <td>0.709819</td>\n",
       "      <td>68be06ca386d4c31939f3a4f0e3dd783</td>\n",
       "      <td>0.377778</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.445783</td>\n",
       "      <td>0.793747</td>\n",
       "      <td>0610b486422d4921ae7d2bf64640c50b</td>\n",
       "      <td>0.911111</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.445783</td>\n",
       "      <td>0.992320</td>\n",
       "      <td>38fe809add3b4fcf9315a9694bb96ff5</td>\n",
       "      <td>0.377778</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.686747</td>\n",
       "      <td>0.756994</td>\n",
       "      <td>78afa995795e4d85b5d9ceeca43f5fef</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.445783</td>\n",
       "      <td>0.804717</td>\n",
       "      <td>a03223e636434f42ac4c3df47e8bac43</td>\n",
       "      <td>0.377778</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age  became_member_on                                id    income  \\\n",
       "0  0.445783          0.709819  68be06ca386d4c31939f3a4f0e3dd783  0.377778   \n",
       "1  0.445783          0.793747  0610b486422d4921ae7d2bf64640c50b  0.911111   \n",
       "2  0.445783          0.992320  38fe809add3b4fcf9315a9694bb96ff5  0.377778   \n",
       "3  0.686747          0.756994  78afa995795e4d85b5d9ceeca43f5fef  0.777778   \n",
       "4  0.445783          0.804717  a03223e636434f42ac4c3df47e8bac43  0.377778   \n",
       "\n",
       "   gender_F  gender_M  gender_O  gender_nan  \n",
       "0         0         0         0           1  \n",
       "1         1         0         0           0  \n",
       "2         0         0         0           1  \n",
       "3         1         0         0           0  \n",
       "4         0         0         0           1  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile_df = process_profile()\n",
    "profile_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcript Dataset\n",
    "\n",
    "Lets preprocess transcript data. We can see there are three types of event: offer received, offer viewed and offer completed. And we see a complex value column of which the content may vary as event type. So, we split value column into offer_id, amount and reward columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person</th>\n",
       "      <th>offer_id</th>\n",
       "      <th>offer_type</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>amount</th>\n",
       "      <th>receive_time</th>\n",
       "      <th>view_time</th>\n",
       "      <th>complete_time</th>\n",
       "      <th>expected_complete_time</th>\n",
       "      <th>is_in_expected_complete_time</th>\n",
       "      <th>is_enough_amount</th>\n",
       "      <th>is_view_event</th>\n",
       "      <th>is_complete_event</th>\n",
       "      <th>is_complete</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0009655768c64bdeb2e877511632db8f</td>\n",
       "      <td>5a8bc65990b245e5a138643cd4eb9837</td>\n",
       "      <td>informational</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.16</td>\n",
       "      <td>168</td>\n",
       "      <td>192.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>240.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0009655768c64bdeb2e877511632db8f</td>\n",
       "      <td>3f207df678b143eea3cee63160fa8bed</td>\n",
       "      <td>informational</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.57</td>\n",
       "      <td>336</td>\n",
       "      <td>372.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>432.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0009655768c64bdeb2e877511632db8f</td>\n",
       "      <td>f19421c1d4aa40978ebb69ca19b0e20d</td>\n",
       "      <td>bogo</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.57</td>\n",
       "      <td>408</td>\n",
       "      <td>456.0</td>\n",
       "      <td>414.0</td>\n",
       "      <td>528.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0009655768c64bdeb2e877511632db8f</td>\n",
       "      <td>fafdcd668e3743c1bb461111dcafc2a4</td>\n",
       "      <td>discount</td>\n",
       "      <td>10.0</td>\n",
       "      <td>14.11</td>\n",
       "      <td>504</td>\n",
       "      <td>540.0</td>\n",
       "      <td>528.0</td>\n",
       "      <td>744.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0009655768c64bdeb2e877511632db8f</td>\n",
       "      <td>2906b810c7d4411798c6938adc9daaa5</td>\n",
       "      <td>discount</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.27</td>\n",
       "      <td>576</td>\n",
       "      <td>NaN</td>\n",
       "      <td>576.0</td>\n",
       "      <td>744.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             person                          offer_id  \\\n",
       "0  0009655768c64bdeb2e877511632db8f  5a8bc65990b245e5a138643cd4eb9837   \n",
       "1  0009655768c64bdeb2e877511632db8f  3f207df678b143eea3cee63160fa8bed   \n",
       "2  0009655768c64bdeb2e877511632db8f  f19421c1d4aa40978ebb69ca19b0e20d   \n",
       "3  0009655768c64bdeb2e877511632db8f  fafdcd668e3743c1bb461111dcafc2a4   \n",
       "4  0009655768c64bdeb2e877511632db8f  2906b810c7d4411798c6938adc9daaa5   \n",
       "\n",
       "      offer_type  difficulty  amount  receive_time  view_time  complete_time  \\\n",
       "0  informational         0.0   22.16           168      192.0            NaN   \n",
       "1  informational         0.0    8.57           336      372.0            NaN   \n",
       "2           bogo         5.0    8.57           408      456.0          414.0   \n",
       "3       discount        10.0   14.11           504      540.0          528.0   \n",
       "4       discount        10.0   10.27           576        NaN          576.0   \n",
       "\n",
       "   expected_complete_time  is_in_expected_complete_time  is_enough_amount  \\\n",
       "0                   240.0                         False              True   \n",
       "1                   432.0                         False              True   \n",
       "2                   528.0                          True              True   \n",
       "3                   744.0                          True              True   \n",
       "4                   744.0                          True              True   \n",
       "\n",
       "   is_view_event  is_complete_event  is_complete  \n",
       "0           True              False         True  \n",
       "1           True              False         True  \n",
       "2           True               True         True  \n",
       "3           True               True         True  \n",
       "4          False               True        False  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see scripts/process_transcript.py to learn how data preprocessing works\n",
    "transcript_df = pd.read_csv('data/processed_transcript.csv')\n",
    "transcript_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge three tables\n",
    "df = transcript_df[[\"person\", \"offer_id\", \"is_complete\"]].\\\n",
    "merge(profile_df, left_on=\"person\", right_on=\"id\").drop([\"id\", \"person\"], axis=1).\\\n",
    "merge(portfolio_df, left_on=\"offer_id\", right_on=\"id\").drop([\"id\", \"offer_id\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make is_complete 1 or 0\n",
    "df.is_complete = df.is_complete.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76277, 20)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_complete</th>\n",
       "      <th>age</th>\n",
       "      <th>became_member_on</th>\n",
       "      <th>income</th>\n",
       "      <th>gender_F</th>\n",
       "      <th>gender_M</th>\n",
       "      <th>gender_O</th>\n",
       "      <th>gender_nan</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>duration</th>\n",
       "      <th>reward</th>\n",
       "      <th>offer_type_bogo</th>\n",
       "      <th>offer_type_discount</th>\n",
       "      <th>offer_type_informational</th>\n",
       "      <th>channel_email</th>\n",
       "      <th>channel_web</th>\n",
       "      <th>channel_mobile</th>\n",
       "      <th>channel_social</th>\n",
       "      <th>reward_difficulty</th>\n",
       "      <th>difficulty_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.180723</td>\n",
       "      <td>0.747120</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.265060</td>\n",
       "      <td>0.891388</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.493976</td>\n",
       "      <td>0.520570</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.072289</td>\n",
       "      <td>0.658804</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.096386</td>\n",
       "      <td>0.780581</td>\n",
       "      <td>0.477778</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_complete       age  became_member_on    income  gender_F  gender_M  \\\n",
       "0            1  0.180723          0.747120  0.466667         0         1   \n",
       "1            0  0.265060          0.891388  0.300000         0         0   \n",
       "2            1  0.493976          0.520570  0.666667         1         0   \n",
       "3            1  0.072289          0.658804  0.333333         1         0   \n",
       "4            1  0.096386          0.780581  0.477778         1         0   \n",
       "\n",
       "   gender_O  gender_nan  difficulty  duration  reward  offer_type_bogo  \\\n",
       "0         0           0         0.0       0.0     0.0                0   \n",
       "1         1           0         0.0       0.0     0.0                0   \n",
       "2         0           0         0.0       0.0     0.0                0   \n",
       "3         0           0         0.0       0.0     0.0                0   \n",
       "4         0           0         0.0       0.0     0.0                0   \n",
       "\n",
       "   offer_type_discount  offer_type_informational  channel_email  channel_web  \\\n",
       "0                    0                         1              1            0   \n",
       "1                    0                         1              1            0   \n",
       "2                    0                         1              1            0   \n",
       "3                    0                         1              1            0   \n",
       "4                    0                         1              1            0   \n",
       "\n",
       "   channel_mobile  channel_social  reward_difficulty  difficulty_duration  \n",
       "0               1               1                0.0                  0.0  \n",
       "1               1               1                0.0                  0.0  \n",
       "2               1               1                0.0                  0.0  \n",
       "3               1               1                0.0                  0.0  \n",
       "4               1               1                0.0                  0.0  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train/test\n",
    "def train_test_split(df, train_frac= 0.7, seed=666):\n",
    "    '''Shuffle the data and randomly split into train and test sets;\n",
    "       separate the class labels (the column in transaction_df) from the features.\n",
    "       :param df: Dataframe of all credit card transaction data\n",
    "       :param train_frac: The decimal fraction of data that should be training data\n",
    "       :param seed: Random seed for shuffling and reproducibility, default = 1\n",
    "       :return: Two tuples (in order): (train_features, train_labels), (test_features, test_labels)\n",
    "       '''\n",
    "    \n",
    "    # shuffle and split the data\n",
    "    df_matrix = df.values\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(df_matrix)\n",
    "    \n",
    "    train_size = int(df_matrix.shape[0] * train_frac)\n",
    "    train_features = df_matrix[:train_size, 1:]\n",
    "    train_labels = df_matrix[:train_size, 0]\n",
    "    \n",
    "    test_features = df_matrix[train_size:, 1:]\n",
    "    test_labels = df_matrix[train_size:, 0]\n",
    "    \n",
    "    return (train_features, train_labels), (test_features, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_x, train_y), (test_x, test_y) = train_test_split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((53393, 19), (53393,))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((22884, 19), (22884,))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the data locally and upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([\n",
    "    pd.DataFrame(train_y), \n",
    "    pd.DataFrame(train_x)\n",
    "], axis=1).to_csv(\"data/train.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([\n",
    "    pd.DataFrame(test_y), \n",
    "    pd.DataFrame(test_x)\n",
    "], axis=1).to_csv(\"data/test_full.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(test_x).to_csv(\"data/test.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "# This is an object that represents the SageMaker session that we are currently operating in. This\n",
    "# object contains some useful information that we will need to access later such as our region.\n",
    "session = sagemaker.Session()\n",
    "\n",
    "# This is an object that represents the IAM role that we are currently assigned. When we construct\n",
    "# and launch the training job later we will need to tell it what IAM role it should have. Since our\n",
    "# use case is relatively simple we will simply assign the training job the role we currently have.\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'starbucks-xgboost'\n",
    "\n",
    "train_location = session.upload_data(\"data/train.csv\", key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-west-2-203336335427/starbucks-xgboost/train.csv'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-west-2-203336335427/starbucks-xgboost/test.csv'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_location = session.upload_data(\"data/test.csv\", key_prefix=prefix)\n",
    "test_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-27 00:24:32 Starting - Starting the training job...\n",
      "2019-06-27 00:24:33 Starting - Launching requested ML instances......\n",
      "2019-06-27 00:25:36 Starting - Preparing the instances for training...\n",
      "2019-06-27 00:26:29 Downloading - Downloading input data...\n",
      "2019-06-27 00:26:48 Training - Downloading the training image..\n",
      "\u001b[31mArguments: train\u001b[0m\n",
      "\u001b[31m[2019-06-27:00:27:06:INFO] Running standalone xgboost training.\u001b[0m\n",
      "\u001b[31m[2019-06-27:00:27:06:INFO] Path /opt/ml/input/data/validation does not exist!\u001b[0m\n",
      "\u001b[31m[2019-06-27:00:27:06:INFO] File size need to be processed in the node: 7.56mb. Available memory size in the node: 8465.25mb\u001b[0m\n",
      "\u001b[31m[2019-06-27:00:27:06:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[31m[00:27:06] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[31m[00:27:06] 53393x19 matrix with 1014467 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[31m[00:27:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[0]#011train-error:0.306726\u001b[0m\n",
      "\u001b[31mWill train until train-error hasn't improved in 10 rounds.\u001b[0m\n",
      "\u001b[31m[00:27:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[1]#011train-error:0.303692\u001b[0m\n",
      "\u001b[31m[00:27:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[2]#011train-error:0.302437\u001b[0m\n",
      "\u001b[31m[00:27:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[3]#011train-error:0.298728\u001b[0m\n",
      "\u001b[31m[00:27:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[4]#011train-error:0.291574\u001b[0m\n",
      "\u001b[31m[00:27:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[5]#011train-error:0.291012\u001b[0m\n",
      "\u001b[31m[00:27:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 44 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[6]#011train-error:0.290562\u001b[0m\n",
      "\u001b[31m[00:27:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[7]#011train-error:0.290094\u001b[0m\n",
      "\u001b[31m[00:27:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[8]#011train-error:0.287585\u001b[0m\n",
      "\u001b[31m[00:27:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[9]#011train-error:0.288221\u001b[0m\n",
      "\u001b[31m[00:27:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[10]#011train-error:0.286536\u001b[0m\n",
      "\u001b[31m[00:27:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 40 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[11]#011train-error:0.28676\u001b[0m\n",
      "\u001b[31m[00:27:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[12]#011train-error:0.286629\u001b[0m\n",
      "\u001b[31m[00:27:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[13]#011train-error:0.28382\u001b[0m\n",
      "\u001b[31m[00:27:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[14]#011train-error:0.28264\u001b[0m\n",
      "\u001b[31m[00:27:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[15]#011train-error:0.282116\u001b[0m\n",
      "\u001b[31m[00:27:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 30 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[16]#011train-error:0.281835\u001b[0m\n",
      "\u001b[31m[00:27:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[17]#011train-error:0.280655\u001b[0m\n",
      "\u001b[31m[00:27:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[18]#011train-error:0.280243\u001b[0m\n",
      "\u001b[31m[00:27:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[19]#011train-error:0.279812\u001b[0m\n",
      "\u001b[31m[00:27:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[20]#011train-error:0.27955\u001b[0m\n",
      "\u001b[31m[00:27:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[21]#011train-error:0.279512\u001b[0m\n",
      "\u001b[31m[00:27:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[22]#011train-error:0.279119\u001b[0m\n",
      "\u001b[31m[00:27:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[23]#011train-error:0.278875\u001b[0m\n",
      "\u001b[31m[00:27:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[24]#011train-error:0.278314\u001b[0m\n",
      "\u001b[31m[00:27:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[25]#011train-error:0.277864\u001b[0m\n",
      "\u001b[31m[00:27:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[26]#011train-error:0.277059\u001b[0m\n",
      "\u001b[31m[00:27:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[27]#011train-error:0.276909\u001b[0m\n",
      "\u001b[31m[00:27:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[28]#011train-error:0.27674\u001b[0m\n",
      "\u001b[31m[00:27:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[29]#011train-error:0.276459\u001b[0m\n",
      "\u001b[31m[00:27:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 44 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[30]#011train-error:0.276291\u001b[0m\n",
      "\u001b[31m[00:27:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[31]#011train-error:0.276216\u001b[0m\n",
      "\u001b[31m[00:27:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[32]#011train-error:0.276253\u001b[0m\n",
      "\u001b[31m[00:27:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[33]#011train-error:0.276029\u001b[0m\n",
      "\u001b[31m[00:27:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[34]#011train-error:0.275785\u001b[0m\n",
      "\u001b[31m[00:27:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 40 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[35]#011train-error:0.275223\u001b[0m\n",
      "\u001b[31m[00:27:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[36]#011train-error:0.274924\u001b[0m\n",
      "\u001b[31m[00:27:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[37]#011train-error:0.274605\u001b[0m\n",
      "\u001b[31m[00:27:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[38]#011train-error:0.274399\u001b[0m\n",
      "\u001b[31m[00:27:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[39]#011train-error:0.274193\u001b[0m\n",
      "\u001b[31m[00:27:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[40]#011train-error:0.273837\u001b[0m\n",
      "\u001b[31m[00:27:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[41]#011train-error:0.273238\u001b[0m\n",
      "\u001b[31m[00:27:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[42]#011train-error:0.27292\u001b[0m\n",
      "\u001b[31m[00:27:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[43]#011train-error:0.272264\u001b[0m\n",
      "\u001b[31m[00:27:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[44]#011train-error:0.272208\u001b[0m\n",
      "\u001b[31m[00:27:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[45]#011train-error:0.272058\u001b[0m\n",
      "\u001b[31m[00:27:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[46]#011train-error:0.272058\u001b[0m\n",
      "\u001b[31m[00:27:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[47]#011train-error:0.271721\u001b[0m\n",
      "\u001b[31m[00:27:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[48]#011train-error:0.271609\u001b[0m\n",
      "\u001b[31m[00:27:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[49]#011train-error:0.271384\u001b[0m\n",
      "\u001b[31m[00:27:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 26 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[50]#011train-error:0.271047\u001b[0m\n",
      "\u001b[31m[00:27:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[51]#011train-error:0.270972\u001b[0m\n",
      "\u001b[31m[00:27:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[52]#011train-error:0.271047\u001b[0m\n",
      "\u001b[31m[00:27:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[53]#011train-error:0.270841\u001b[0m\n",
      "\u001b[31m[00:27:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[54]#011train-error:0.270485\u001b[0m\n",
      "\u001b[31m[00:27:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[55]#011train-error:0.270504\u001b[0m\n",
      "\u001b[31m[00:27:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[56]#011train-error:0.270279\u001b[0m\n",
      "\u001b[31m[00:27:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[57]#011train-error:0.270241\u001b[0m\n",
      "\u001b[31m[00:27:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 18 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[31m[58]#011train-error:0.270241\u001b[0m\n",
      "\u001b[31m[00:27:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[59]#011train-error:0.270054\u001b[0m\n",
      "\u001b[31m[00:27:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[60]#011train-error:0.269942\u001b[0m\n",
      "\u001b[31m[00:27:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[61]#011train-error:0.270035\u001b[0m\n",
      "\u001b[31m[00:27:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[62]#011train-error:0.269904\u001b[0m\n",
      "\u001b[31m[00:27:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[63]#011train-error:0.269361\u001b[0m\n",
      "\u001b[31m[00:27:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[64]#011train-error:0.269567\u001b[0m\n",
      "\u001b[31m[00:27:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[65]#011train-error:0.269286\u001b[0m\n",
      "\u001b[31m[00:27:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[66]#011train-error:0.26893\u001b[0m\n",
      "\u001b[31m[00:27:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[67]#011train-error:0.268237\u001b[0m\n",
      "\u001b[31m[00:27:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[68]#011train-error:0.268069\u001b[0m\n",
      "\u001b[31m[00:27:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[69]#011train-error:0.268331\u001b[0m\n",
      "\u001b[31m[00:27:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[70]#011train-error:0.267732\u001b[0m\n",
      "\u001b[31m[00:27:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[71]#011train-error:0.267676\u001b[0m\n",
      "\u001b[31m[00:27:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 40 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[72]#011train-error:0.267245\u001b[0m\n",
      "\u001b[31m[00:27:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[73]#011train-error:0.26732\u001b[0m\n",
      "\u001b[31m[00:27:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 40 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[74]#011train-error:0.267282\u001b[0m\n",
      "\u001b[31m[00:27:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[75]#011train-error:0.266739\u001b[0m\n",
      "\u001b[31m[00:27:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[76]#011train-error:0.266627\u001b[0m\n",
      "\u001b[31m[00:27:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[77]#011train-error:0.266645\u001b[0m\n",
      "\u001b[31m[00:27:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[78]#011train-error:0.266496\u001b[0m\n",
      "\u001b[31m[00:27:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[79]#011train-error:0.266121\u001b[0m\n",
      "\u001b[31m[00:27:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 32 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[80]#011train-error:0.265672\u001b[0m\n",
      "\u001b[31m[00:27:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 26 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[81]#011train-error:0.26599\u001b[0m\n",
      "\u001b[31m[00:27:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[82]#011train-error:0.265934\u001b[0m\n",
      "\u001b[31m[00:27:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 44 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[83]#011train-error:0.265971\u001b[0m\n",
      "\u001b[31m[00:27:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[84]#011train-error:0.265578\u001b[0m\n",
      "\u001b[31m[00:27:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 26 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[31m[85]#011train-error:0.265578\u001b[0m\n",
      "\u001b[31m[00:27:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[86]#011train-error:0.265409\u001b[0m\n",
      "\u001b[31m[00:27:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[87]#011train-error:0.265409\u001b[0m\n",
      "\u001b[31m[00:27:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 30 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[88]#011train-error:0.265934\u001b[0m\n",
      "\u001b[31m[00:27:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[89]#011train-error:0.265372\u001b[0m\n",
      "\u001b[31m[00:27:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[90]#011train-error:0.265035\u001b[0m\n",
      "\u001b[31m[00:27:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[91]#011train-error:0.265016\u001b[0m\n",
      "\u001b[31m[00:27:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[92]#011train-error:0.264885\u001b[0m\n",
      "\u001b[31m[00:27:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[93]#011train-error:0.264529\u001b[0m\n",
      "\u001b[31m[00:27:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[94]#011train-error:0.264492\u001b[0m\n",
      "\u001b[31m[00:27:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[95]#011train-error:0.264023\u001b[0m\n",
      "\u001b[31m[00:27:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[96]#011train-error:0.264042\u001b[0m\n",
      "\u001b[31m[00:27:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[97]#011train-error:0.263817\u001b[0m\n",
      "\u001b[31m[00:27:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 28 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[98]#011train-error:0.263668\u001b[0m\n",
      "\u001b[31m[00:27:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 40 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[99]#011train-error:0.263686\u001b[0m\n",
      "\u001b[31m[00:27:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[100]#011train-error:0.26363\u001b[0m\n",
      "\u001b[31m[00:27:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[101]#011train-error:0.263705\u001b[0m\n",
      "\u001b[31m[00:27:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[102]#011train-error:0.263443\u001b[0m\n",
      "\u001b[31m[00:27:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[103]#011train-error:0.263162\u001b[0m\n",
      "\u001b[31m[00:27:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[104]#011train-error:0.263031\u001b[0m\n",
      "\u001b[31m[00:27:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[105]#011train-error:0.262712\u001b[0m\n",
      "\u001b[31m[00:27:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[106]#011train-error:0.262394\u001b[0m\n",
      "\u001b[31m[00:27:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[107]#011train-error:0.261982\u001b[0m\n",
      "\u001b[31m[00:27:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 16 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[31m[108]#011train-error:0.261926\u001b[0m\n",
      "\u001b[31m[00:27:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[109]#011train-error:0.261776\u001b[0m\n",
      "\u001b[31m[00:27:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[110]#011train-error:0.261701\u001b[0m\n",
      "\u001b[31m[00:27:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[111]#011train-error:0.261982\u001b[0m\n",
      "\u001b[31m[00:27:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[112]#011train-error:0.261907\u001b[0m\n",
      "\u001b[31m[00:27:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[113]#011train-error:0.26157\u001b[0m\n",
      "\u001b[31m[00:27:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[114]#011train-error:0.261664\u001b[0m\n",
      "\u001b[31m[00:27:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 30 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[115]#011train-error:0.261682\u001b[0m\n",
      "\u001b[31m[00:27:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 28 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[116]#011train-error:0.261326\u001b[0m\n",
      "\u001b[31m[00:27:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[117]#011train-error:0.261064\u001b[0m\n",
      "\u001b[31m[00:27:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[118]#011train-error:0.260746\u001b[0m\n",
      "\u001b[31m[00:27:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[119]#011train-error:0.260671\u001b[0m\n",
      "\u001b[31m[00:27:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[120]#011train-error:0.260708\u001b[0m\n",
      "\u001b[31m[00:27:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[121]#011train-error:0.260802\u001b[0m\n",
      "\u001b[31m[00:27:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 50 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[31m[122]#011train-error:0.260839\u001b[0m\n",
      "\u001b[31m[00:27:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[123]#011train-error:0.260708\u001b[0m\n",
      "\u001b[31m[00:27:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 32 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[31m[124]#011train-error:0.260727\u001b[0m\n",
      "\u001b[31m[00:27:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[125]#011train-error:0.260577\u001b[0m\n",
      "\u001b[31m[00:27:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 32 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[31m[126]#011train-error:0.260727\u001b[0m\n",
      "\u001b[31m[00:27:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[127]#011train-error:0.260371\u001b[0m\n",
      "\u001b[31m[00:27:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[128]#011train-error:0.260446\u001b[0m\n",
      "\u001b[31m[00:27:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[129]#011train-error:0.260465\u001b[0m\n",
      "\u001b[31m[00:27:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[130]#011train-error:0.260015\u001b[0m\n",
      "\u001b[31m[00:27:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[131]#011train-error:0.25994\u001b[0m\n",
      "\u001b[31m[00:27:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[132]#011train-error:0.259903\u001b[0m\n",
      "\u001b[31m[00:27:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 34 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[133]#011train-error:0.259847\u001b[0m\n",
      "\u001b[31m[00:27:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 18 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[31m[134]#011train-error:0.259903\u001b[0m\n",
      "\u001b[31m[00:27:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[135]#011train-error:0.259678\u001b[0m\n",
      "\u001b[31m[00:27:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 32 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[136]#011train-error:0.259491\u001b[0m\n",
      "\u001b[31m[00:27:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 32 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[137]#011train-error:0.259716\u001b[0m\n",
      "\u001b[31m[00:27:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 26 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[138]#011train-error:0.259528\u001b[0m\n",
      "\u001b[31m[00:27:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 44 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[139]#011train-error:0.259285\u001b[0m\n",
      "\u001b[31m[00:27:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[140]#011train-error:0.258929\u001b[0m\n",
      "\u001b[31m[00:27:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 30 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[31m[141]#011train-error:0.258835\u001b[0m\n",
      "\u001b[31m[00:27:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 26 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[142]#011train-error:0.258761\u001b[0m\n",
      "\u001b[31m[00:27:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[143]#011train-error:0.258423\u001b[0m\n",
      "\u001b[31m[00:27:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 26 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[144]#011train-error:0.258536\u001b[0m\n",
      "\u001b[31m[00:27:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 40 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[145]#011train-error:0.258423\u001b[0m\n",
      "\u001b[31m[00:27:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[146]#011train-error:0.258217\u001b[0m\n",
      "\u001b[31m[00:27:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[147]#011train-error:0.257862\u001b[0m\n",
      "\u001b[31m[00:27:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[148]#011train-error:0.257375\u001b[0m\n",
      "\u001b[31m[00:27:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 26 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[149]#011train-error:0.257431\u001b[0m\n",
      "\u001b[31m[00:27:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[150]#011train-error:0.2573\u001b[0m\n",
      "\u001b[31m[00:27:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 28 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[151]#011train-error:0.256963\u001b[0m\n",
      "\u001b[31m[00:27:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[152]#011train-error:0.256682\u001b[0m\n",
      "\u001b[31m[00:27:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 28 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[153]#011train-error:0.256888\u001b[0m\n",
      "\u001b[31m[00:27:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 26 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[154]#011train-error:0.256963\u001b[0m\n",
      "\u001b[31m[00:27:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 26 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[155]#011train-error:0.256682\u001b[0m\n",
      "\u001b[31m[00:27:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[156]#011train-error:0.256363\u001b[0m\n",
      "\u001b[31m[00:27:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 26 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[31m[157]#011train-error:0.256363\u001b[0m\n",
      "\u001b[31m[00:27:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[158]#011train-error:0.256363\u001b[0m\n",
      "\u001b[31m[00:27:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[159]#011train-error:0.256438\u001b[0m\n",
      "\u001b[31m[00:27:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 28 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[31m[160]#011train-error:0.256419\u001b[0m\n",
      "\u001b[31m[00:27:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 26 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[161]#011train-error:0.256438\u001b[0m\n",
      "\u001b[31m[00:27:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 30 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[162]#011train-error:0.256438\u001b[0m\n",
      "\u001b[31m[00:27:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[163]#011train-error:0.256213\u001b[0m\n",
      "\u001b[31m[00:27:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 26 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[164]#011train-error:0.256382\u001b[0m\n",
      "\u001b[31m[00:27:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[165]#011train-error:0.25655\u001b[0m\n",
      "\u001b[31m[00:27:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[166]#011train-error:0.256494\u001b[0m\n",
      "\u001b[31m[00:27:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[167]#011train-error:0.256363\u001b[0m\n",
      "\u001b[31m[00:27:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 22 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[31m[168]#011train-error:0.256401\u001b[0m\n",
      "\u001b[31m[00:27:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[169]#011train-error:0.256232\u001b[0m\n",
      "\u001b[31m[00:27:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 28 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[170]#011train-error:0.256082\u001b[0m\n",
      "\u001b[31m[00:27:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[171]#011train-error:0.255801\u001b[0m\n",
      "\u001b[31m[00:27:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[172]#011train-error:0.255689\u001b[0m\n",
      "\u001b[31m[00:27:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 22 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[31m[173]#011train-error:0.255783\u001b[0m\n",
      "\u001b[31m[00:27:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 38 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[174]#011train-error:0.25582\u001b[0m\n",
      "\u001b[31m[00:27:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[175]#011train-error:0.255445\u001b[0m\n",
      "\u001b[31m[00:27:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 32 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[31m[176]#011train-error:0.255577\u001b[0m\n",
      "\u001b[31m[00:27:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 30 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[177]#011train-error:0.255352\u001b[0m\n",
      "\u001b[31m[00:27:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 26 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[31m[178]#011train-error:0.255333\u001b[0m\n",
      "\u001b[31m[00:27:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 20 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[31m[179]#011train-error:0.255296\u001b[0m\n",
      "\u001b[31m[00:27:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[180]#011train-error:0.255015\u001b[0m\n",
      "\u001b[31m[00:27:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[181]#011train-error:0.254734\u001b[0m\n",
      "\u001b[31m[00:27:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[182]#011train-error:0.254621\u001b[0m\n",
      "\u001b[31m[00:27:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[183]#011train-error:0.254359\u001b[0m\n",
      "\u001b[31m[00:27:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 28 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[184]#011train-error:0.254097\u001b[0m\n",
      "\u001b[31m[00:27:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[185]#011train-error:0.25391\u001b[0m\n",
      "\u001b[31m[00:27:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[186]#011train-error:0.253872\u001b[0m\n",
      "\u001b[31m[00:27:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 40 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[187]#011train-error:0.25346\u001b[0m\n",
      "\u001b[31m[00:27:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[188]#011train-error:0.253498\u001b[0m\n",
      "\u001b[31m[00:27:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 26 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[189]#011train-error:0.253516\u001b[0m\n",
      "\u001b[31m[00:27:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 26 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[190]#011train-error:0.253629\u001b[0m\n",
      "\u001b[31m[00:27:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[191]#011train-error:0.253685\u001b[0m\n",
      "\u001b[31m[00:27:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[192]#011train-error:0.25376\u001b[0m\n",
      "\u001b[31m[00:27:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 26 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[31m[193]#011train-error:0.253535\u001b[0m\n",
      "\u001b[31m[00:27:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[194]#011train-error:0.253516\u001b[0m\n",
      "\u001b[31m[00:27:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[195]#011train-error:0.253629\u001b[0m\n",
      "\u001b[31m[00:27:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[196]#011train-error:0.253647\u001b[0m\n",
      "\u001b[31m[00:27:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[31m[197]#011train-error:0.25361\u001b[0m\n",
      "\u001b[31mStopping. Best iteration:\u001b[0m\n",
      "\u001b[31m[187]#011train-error:0.25346\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2019-06-27 00:27:23 Uploading - Uploading generated training model\n",
      "2019-06-27 00:27:23 Completed - Training job completed\n",
      "Billable seconds: 54\n"
     ]
    }
   ],
   "source": [
    "# As stated above, we use this utility method to construct the image name for the training container.\n",
    "container = get_image_uri(session.boto_region_name, 'xgboost')\n",
    "\n",
    "# Now that we know which container to use, we can construct the estimator object.\n",
    "xgb = sagemaker.estimator.Estimator(\n",
    "    container, # The name of the training container\n",
    "    role,      # The IAM role to use (our current role in this case)\n",
    "    train_instance_count=1, # The number of instances to use for training\n",
    "    train_instance_type='ml.m4.xlarge', # The type of instance ot use for training\n",
    "    output_path=f's3://{session.default_bucket()}/{prefix}/output',\n",
    "                                        # Where to save the output (the model artifacts)\n",
    "    sagemaker_session=session) # The current SageMaker session\n",
    "\n",
    "\n",
    "# set hyperparameters\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        objective='binary:logistic',\n",
    "                        early_stopping_rounds=10,\n",
    "                        num_round=200)\n",
    "\n",
    "\n",
    "# This is a wrapper around the location of our train and validation data, to make sure that SageMaker\n",
    "# knows our data is in csv format.\n",
    "s3_input_train = sagemaker.s3_input(s3_data=train_location, content_type='csv')\n",
    "\n",
    "xgb.fit({'train': s3_input_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................................!\n"
     ]
    }
   ],
   "source": [
    "xgb_transformer = xgb.transformer(instance_count = 1, instance_type = 'ml.m4.xlarge')\n",
    "xgb_transformer.transform(test_location, content_type='text/csv', split_type='Line')\n",
    "xgb_transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 256.0 KiB/334.6 KiB (3.6 MiB/s) with 1 file(s) remaining\r",
      "Completed 334.6 KiB/334.6 KiB (4.6 MiB/s) with 1 file(s) remaining\r",
      "download: s3://sagemaker-us-west-2-203336335427/xgboost-2019-06-27-00-29-09-523/test.csv.out to data/test.csv.out\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --recursive $xgb_transformer.output_path $\"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.read_csv(\"data/test.csv.out\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [round(num) for num in predictions.squeeze().values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7212025869603216"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.682207611077904"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(test_y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7159450152591236"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(test_y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Neural Network Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "# session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# create an S3 bucket\n",
    "bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"starbucks-pytorch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import a PyTorch wrapper\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# specify an output path\n",
    "output_path = f\"s3://{bucket}/{prefix}\"\n",
    "\n",
    "# instantiate a pytorch estimator\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"pytorch\",\n",
    "    role=role,\n",
    "    framework_version=\"1.0\",\n",
    "    train_instance_count=1,\n",
    "    train_instance_type=\"ml.p2.xlarge\",\n",
    "    output_path=output_path,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    hyperparameters={\n",
    "        \"input_features\": 19,\n",
    "        \"hidden_dim\": 30, \n",
    "        \"output_dim\": 1,\n",
    "        \"epochs\": 100\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-28 20:21:10 Starting - Starting the training job...\n",
      "2019-06-28 20:21:11 Starting - Launching requested ML instances......\n",
      "2019-06-28 20:22:40 Starting - Preparing the instances for training............\n",
      "2019-06-28 20:24:21 Downloading - Downloading input data...\n",
      "2019-06-28 20:24:42 Training - Downloading the training image...\n",
      "2019-06-28 20:25:31 Training - Training image download completed. Training in progress.\n",
      "\u001b[31mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[31mbash: no job control in this shell\u001b[0m\n",
      "\u001b[31m2019-06-28 20:25:32,979 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[31m2019-06-28 20:25:33,007 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[31m2019-06-28 20:25:36,033 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[31m2019-06-28 20:25:36,328 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[31mGenerating setup.py\u001b[0m\n",
      "\u001b[31m2019-06-28 20:25:36,328 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[31m2019-06-28 20:25:36,328 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[31m2019-06-28 20:25:36,329 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[31m/usr/bin/python -m pip install -U . \u001b[0m\n",
      "\u001b[31mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[31mBuilding wheels for collected packages: train\n",
      "  Running setup.py bdist_wheel for train: started\n",
      "  Running setup.py bdist_wheel for train: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-f91n9yub/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[31mSuccessfully built train\u001b[0m\n",
      "\u001b[31mInstalling collected packages: train\u001b[0m\n",
      "\u001b[31mSuccessfully installed train-1.0.0\u001b[0m\n",
      "\u001b[31mYou are using pip version 18.1, however version 19.1.1 is available.\u001b[0m\n",
      "\u001b[31mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[31m2019-06-28 20:25:38,321 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[31mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[31m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"hidden_dim\": 30,\n",
      "        \"input_features\": 19,\n",
      "        \"epochs\": 100,\n",
      "        \"output_dim\": 1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-pytorch-2019-06-28-20-21-09-877\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-203336335427/sagemaker-pytorch-2019-06-28-20-21-09-877/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[31m}\n",
      "\u001b[0m\n",
      "\u001b[31mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[31mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[31mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[31mSM_HPS={\"epochs\":100,\"hidden_dim\":30,\"input_features\":19,\"output_dim\":1}\u001b[0m\n",
      "\u001b[31mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[31mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[31mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[31mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[31mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[31mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[31mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[31mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[31mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[31mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[31mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[31mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[31mSM_MODULE_DIR=s3://sagemaker-us-west-2-203336335427/sagemaker-pytorch-2019-06-28-20-21-09-877/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[31mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":100,\"hidden_dim\":30,\"input_features\":19,\"output_dim\":1},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-pytorch-2019-06-28-20-21-09-877\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-203336335427/sagemaker-pytorch-2019-06-28-20-21-09-877/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[31mSM_USER_ARGS=[\"--epochs\",\"100\",\"--hidden_dim\",\"30\",\"--input_features\",\"19\",\"--output_dim\",\"1\"]\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[31mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[31mSM_HP_HIDDEN_DIM=30\u001b[0m\n",
      "\u001b[31mSM_HP_INPUT_FEATURES=19\u001b[0m\n",
      "\u001b[31mSM_HP_EPOCHS=100\u001b[0m\n",
      "\u001b[31mSM_HP_OUTPUT_DIM=1\u001b[0m\n",
      "\u001b[31mPYTHONPATH=/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[0m\n",
      "\u001b[31mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[31m/usr/bin/python -m train --epochs 100 --hidden_dim 30 --input_features 19 --output_dim 1\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[31mUsing device cuda.\u001b[0m\n",
      "\u001b[31mGet train data loader.\u001b[0m\n",
      "\u001b[31mEpoch: 1, Loss: 0.6068326127858421\u001b[0m\n",
      "\u001b[31mEpoch: 2, Loss: 0.5886195298354054\u001b[0m\n",
      "\u001b[31mEpoch: 3, Loss: 0.5834110728196437\u001b[0m\n",
      "\u001b[31mEpoch: 4, Loss: 0.5808417422243495\u001b[0m\n",
      "\u001b[31mEpoch: 5, Loss: 0.5766733737753125\u001b[0m\n",
      "\u001b[31mEpoch: 6, Loss: 0.5743250208959151\u001b[0m\n",
      "\u001b[31mEpoch: 7, Loss: 0.5724724610060565\u001b[0m\n",
      "\u001b[31mEpoch: 8, Loss: 0.5721360138376778\u001b[0m\n",
      "\u001b[31mEpoch: 9, Loss: 0.5697826247257687\u001b[0m\n",
      "\u001b[31mEpoch: 10, Loss: 0.5698020321208886\u001b[0m\n",
      "\u001b[31mEpoch: 11, Loss: 0.5694091414160719\u001b[0m\n",
      "\u001b[31mEpoch: 12, Loss: 0.5691638220980596\u001b[0m\n",
      "\u001b[31mEpoch: 13, Loss: 0.5681786770063839\u001b[0m\n",
      "\u001b[31mEpoch: 14, Loss: 0.5690790921812423\u001b[0m\n",
      "\u001b[31mEpoch: 15, Loss: 0.5671178526115997\u001b[0m\n",
      "\u001b[31mEpoch: 16, Loss: 0.568460351438\u001b[0m\n",
      "\u001b[31mEpoch: 17, Loss: 0.5669309984153129\u001b[0m\n",
      "\u001b[31mEpoch: 18, Loss: 0.566952973294459\u001b[0m\n",
      "\u001b[31mEpoch: 19, Loss: 0.5660670324657741\u001b[0m\n",
      "\u001b[31mEpoch: 20, Loss: 0.5669877148439152\u001b[0m\n",
      "\u001b[31mEpoch: 21, Loss: 0.5666159898349632\u001b[0m\n",
      "\u001b[31mEpoch: 22, Loss: 0.5666461558963699\u001b[0m\n",
      "\u001b[31mEpoch: 23, Loss: 0.5669188752798329\u001b[0m\n",
      "\u001b[31mEpoch: 24, Loss: 0.5654227849221631\u001b[0m\n",
      "\u001b[31mEpoch: 25, Loss: 0.5654794075515833\u001b[0m\n",
      "\u001b[31mEpoch: 26, Loss: 0.5645634544894713\u001b[0m\n",
      "\u001b[31mEpoch: 27, Loss: 0.5647914812108328\u001b[0m\n",
      "\u001b[31mEpoch: 28, Loss: 0.5667220402885689\u001b[0m\n",
      "\u001b[31mEpoch: 29, Loss: 0.5651175315181415\u001b[0m\n",
      "\u001b[31mEpoch: 30, Loss: 0.5664079826004274\u001b[0m\n",
      "\u001b[31mEpoch: 31, Loss: 0.5655647162668938\u001b[0m\n",
      "\u001b[31mEpoch: 32, Loss: 0.5648948577710529\u001b[0m\n",
      "\u001b[31mEpoch: 33, Loss: 0.5654975066973029\u001b[0m\n",
      "\u001b[31mEpoch: 34, Loss: 0.5648939217632629\u001b[0m\n",
      "\u001b[31mEpoch: 35, Loss: 0.5644651772266024\u001b[0m\n",
      "\u001b[31mEpoch: 36, Loss: 0.5650991868074244\u001b[0m\n",
      "\u001b[31mEpoch: 37, Loss: 0.5649909577984712\u001b[0m\n",
      "\u001b[31mEpoch: 38, Loss: 0.5649838880569524\u001b[0m\n",
      "\u001b[31mEpoch: 39, Loss: 0.5646781890747252\u001b[0m\n",
      "\u001b[31mEpoch: 40, Loss: 0.5638303150649597\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mEpoch: 41, Loss: 0.5647252390112323\u001b[0m\n",
      "\u001b[31mEpoch: 42, Loss: 0.5647615809035436\u001b[0m\n",
      "\u001b[31mEpoch: 43, Loss: 0.5647300049216113\u001b[0m\n",
      "\u001b[31mEpoch: 44, Loss: 0.565338591371806\u001b[0m\n",
      "\u001b[31mEpoch: 45, Loss: 0.5640366685775559\u001b[0m\n",
      "\u001b[31mEpoch: 46, Loss: 0.5651824238511284\u001b[0m\n",
      "\u001b[31mEpoch: 47, Loss: 0.5632522821063629\u001b[0m\n",
      "\u001b[31mEpoch: 48, Loss: 0.5636568335083764\u001b[0m\n",
      "\u001b[31mEpoch: 49, Loss: 0.5639813809442833\u001b[0m\n",
      "\u001b[31mEpoch: 50, Loss: 0.5642441238747554\u001b[0m\n",
      "\u001b[31mEpoch: 51, Loss: 0.5634700855289059\u001b[0m\n",
      "\u001b[31mEpoch: 52, Loss: 0.5641865624554372\u001b[0m\n",
      "\u001b[31mEpoch: 53, Loss: 0.5646461112640994\u001b[0m\n",
      "\u001b[31mEpoch: 54, Loss: 0.5637507544000274\u001b[0m\n",
      "\u001b[31mEpoch: 55, Loss: 0.5636580179386148\u001b[0m\n",
      "\u001b[31mEpoch: 56, Loss: 0.5644337049779597\u001b[0m\n",
      "\u001b[31mEpoch: 57, Loss: 0.5639795908962519\u001b[0m\n",
      "\u001b[31mEpoch: 58, Loss: 0.5639862420518746\u001b[0m\n",
      "\u001b[31mEpoch: 59, Loss: 0.5641352043448763\u001b[0m\n",
      "\u001b[31mEpoch: 60, Loss: 0.5638479790521248\u001b[0m\n",
      "\u001b[31mEpoch: 61, Loss: 0.5640465050712506\u001b[0m\n",
      "\u001b[31mEpoch: 62, Loss: 0.5644171724772632\u001b[0m\n",
      "\u001b[31mEpoch: 63, Loss: 0.563724136433463\u001b[0m\n",
      "\u001b[31mEpoch: 64, Loss: 0.5636071085706632\u001b[0m\n",
      "\u001b[31mEpoch: 65, Loss: 0.5643067058943184\u001b[0m\n",
      "\u001b[31mEpoch: 66, Loss: 0.564902804563108\u001b[0m\n",
      "\u001b[31mEpoch: 67, Loss: 0.5642026261695092\u001b[0m\n",
      "\u001b[31mEpoch: 68, Loss: 0.5637049677359701\u001b[0m\n",
      "\u001b[31mEpoch: 69, Loss: 0.5638251073128275\u001b[0m\n",
      "\u001b[31mEpoch: 70, Loss: 0.5630010926461175\u001b[0m\n",
      "\u001b[31mEpoch: 71, Loss: 0.5635810706359841\u001b[0m\n",
      "\u001b[31mEpoch: 72, Loss: 0.5635207537212845\u001b[0m\n",
      "\u001b[31mEpoch: 73, Loss: 0.5633719345826781\u001b[0m\n",
      "\u001b[31mEpoch: 74, Loss: 0.5632529573717368\u001b[0m\n",
      "\u001b[31mEpoch: 75, Loss: 0.5636875245901529\u001b[0m\n",
      "\u001b[31mEpoch: 76, Loss: 0.5634617147318433\u001b[0m\n",
      "\u001b[31mEpoch: 77, Loss: 0.5628983366494992\u001b[0m\n",
      "\u001b[31mEpoch: 78, Loss: 0.5638132956339402\u001b[0m\n",
      "\u001b[31mEpoch: 79, Loss: 0.5632134603929431\u001b[0m\n",
      "\u001b[31mEpoch: 80, Loss: 0.5637478459612484\u001b[0m\n",
      "\u001b[31mEpoch: 81, Loss: 0.5642510952714007\u001b[0m\n",
      "\u001b[31mEpoch: 82, Loss: 0.5638442371054535\u001b[0m\n",
      "\u001b[31mEpoch: 83, Loss: 0.5634408972431881\u001b[0m\n",
      "\u001b[31mEpoch: 84, Loss: 0.5625648510115646\u001b[0m\n",
      "\u001b[31mEpoch: 85, Loss: 0.5634763498905669\u001b[0m\n",
      "\u001b[31mEpoch: 86, Loss: 0.5633655847876929\u001b[0m\n",
      "\u001b[31mEpoch: 87, Loss: 0.5633744156176455\u001b[0m\n",
      "\u001b[31mEpoch: 88, Loss: 0.5632917813817213\u001b[0m\n",
      "\u001b[31mEpoch: 89, Loss: 0.5640144160819635\u001b[0m\n",
      "\u001b[31mEpoch: 90, Loss: 0.5629924608220098\u001b[0m\n",
      "\u001b[31mEpoch: 91, Loss: 0.5631123356018843\u001b[0m\n",
      "\u001b[31mEpoch: 92, Loss: 0.5625729686702682\u001b[0m\n",
      "\u001b[31mEpoch: 93, Loss: 0.5630823653986615\u001b[0m\n",
      "\u001b[31mEpoch: 94, Loss: 0.5632942447920194\u001b[0m\n",
      "\u001b[31mEpoch: 95, Loss: 0.5643239065139705\u001b[0m\n",
      "\u001b[31mEpoch: 96, Loss: 0.5633767781186193\u001b[0m\n",
      "\u001b[31mEpoch: 97, Loss: 0.5631263519605894\u001b[0m\n",
      "\u001b[31mEpoch: 98, Loss: 0.56403734867325\u001b[0m\n",
      "\u001b[31mEpoch: 99, Loss: 0.563468828364816\u001b[0m\n",
      "\u001b[31mEpoch: 100, Loss: 0.5634945600276137\u001b[0m\n",
      "\u001b[31m2019-06-28 20:39:37,141 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2019-06-28 20:40:44 Uploading - Uploading generated training model\n",
      "2019-06-28 20:40:44 Completed - Training job completed\n",
      "Billable seconds: 984\n"
     ]
    }
   ],
   "source": [
    "train_data_path = 's3://sagemaker-us-west-2-203336335427/starbucks-xgboost/train.csv'\n",
    "estimator.fit({'train': train_data_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "# Deploy the trained model\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "model = PyTorchModel(\n",
    "    entry_point=\"predict.py\",\n",
    "    role=role, \n",
    "    framework_version=\"1.0\",\n",
    "    model_data=estimator.model_data,\n",
    "    source_dir=\"pytorch\"\n",
    ")\n",
    "\n",
    "# deploy your model to create a predictor\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type=\"ml.t2.medium\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating the model\n",
    "# read in test data, assuming it is stored locally\n",
    "test_data = pd.read_csv(\"data/test_full.csv\", header=None, names=None)\n",
    "\n",
    "# labels are in the first column\n",
    "test_y = test_data.iloc[:,0]\n",
    "test_x = test_data.iloc[:,1:]\n",
    "test_y_preds = np.hstack(\n",
    "    predictor.predict(test_x.iloc[i: i+1000]).reshape(1, -1).squeeze() \n",
    "    for i in range(0, len(test_x), 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6952660611404713"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the test roc_auc_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(test_y, np.hstack([i.reshape(1, -1).squeeze()] for i in test_y_preds).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lower hidden dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-28 21:01:49 Starting - Starting the training job...\n",
      "2019-06-28 21:01:52 Starting - Launching requested ML instances......\n",
      "2019-06-28 21:03:18 Starting - Preparing the instances for training.........\n",
      "2019-06-28 21:04:48 Downloading - Downloading input data...\n",
      "2019-06-28 21:05:07 Training - Downloading the training image.....\n",
      "\u001b[31mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[31mbash: no job control in this shell\u001b[0m\n",
      "\u001b[31m2019-06-28 21:06:01,468 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[31m2019-06-28 21:06:01,494 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[31m2019-06-28 21:06:01,495 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[31m2019-06-28 21:06:01,743 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[31mGenerating setup.py\u001b[0m\n",
      "\u001b[31m2019-06-28 21:06:01,744 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[31m2019-06-28 21:06:01,744 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[31m2019-06-28 21:06:01,744 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[31m/usr/bin/python -m pip install -U . \u001b[0m\n",
      "\u001b[31mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[31mBuilding wheels for collected packages: train\n",
      "  Running setup.py bdist_wheel for train: started\u001b[0m\n",
      "\u001b[31m  Running setup.py bdist_wheel for train: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-wzeikvdj/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[31mSuccessfully built train\u001b[0m\n",
      "\u001b[31mInstalling collected packages: train\u001b[0m\n",
      "\u001b[31mSuccessfully installed train-1.0.0\u001b[0m\n",
      "\u001b[31mYou are using pip version 18.1, however version 19.1.1 is available.\u001b[0m\n",
      "\u001b[31mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[31m2019-06-28 21:06:04,339 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[31mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[31m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"hidden_dim\": 15,\n",
      "        \"input_features\": 19,\n",
      "        \"epochs\": 100,\n",
      "        \"output_dim\": 1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-pytorch-2019-06-28-21-01-49-440\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-203336335427/sagemaker-pytorch-2019-06-28-21-01-49-440/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[31m}\n",
      "\u001b[0m\n",
      "\u001b[31mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[31mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[31mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[31mSM_HPS={\"epochs\":100,\"hidden_dim\":15,\"input_features\":19,\"output_dim\":1}\u001b[0m\n",
      "\u001b[31mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[31mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[31mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[31mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[31mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[31mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[31mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[31mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[31mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[31mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[31mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[31mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[31mSM_MODULE_DIR=s3://sagemaker-us-west-2-203336335427/sagemaker-pytorch-2019-06-28-21-01-49-440/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[31mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":100,\"hidden_dim\":15,\"input_features\":19,\"output_dim\":1},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-pytorch-2019-06-28-21-01-49-440\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-203336335427/sagemaker-pytorch-2019-06-28-21-01-49-440/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[31mSM_USER_ARGS=[\"--epochs\",\"100\",\"--hidden_dim\",\"15\",\"--input_features\",\"19\",\"--output_dim\",\"1\"]\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[31mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[31mSM_HP_HIDDEN_DIM=15\u001b[0m\n",
      "\u001b[31mSM_HP_INPUT_FEATURES=19\u001b[0m\n",
      "\u001b[31mSM_HP_EPOCHS=100\u001b[0m\n",
      "\u001b[31mSM_HP_OUTPUT_DIM=1\u001b[0m\n",
      "\u001b[31mPYTHONPATH=/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[0m\n",
      "\u001b[31mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[31m/usr/bin/python -m train --epochs 100 --hidden_dim 15 --input_features 19 --output_dim 1\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[31mUsing device cuda.\u001b[0m\n",
      "\u001b[31mGet train data loader.\u001b[0m\n",
      "\n",
      "2019-06-28 21:05:59 Training - Training image download completed. Training in progress.\u001b[31mEpoch: 1, Loss: 0.6096276230970572\u001b[0m\n",
      "\u001b[31mEpoch: 2, Loss: 0.5920321877278415\u001b[0m\n",
      "\u001b[31mEpoch: 3, Loss: 0.5882988782606544\u001b[0m\n",
      "\u001b[31mEpoch: 4, Loss: 0.5846220573654559\u001b[0m\n",
      "\u001b[31mEpoch: 5, Loss: 0.5819676751082533\u001b[0m\n",
      "\u001b[31mEpoch: 6, Loss: 0.5781840415511015\u001b[0m\n",
      "\u001b[31mEpoch: 7, Loss: 0.5793402449719691\u001b[0m\n",
      "\u001b[31mEpoch: 8, Loss: 0.5777780601789666\u001b[0m\n",
      "\u001b[31mEpoch: 9, Loss: 0.5775110768077525\u001b[0m\n",
      "\u001b[31mEpoch: 10, Loss: 0.5767325379289268\u001b[0m\n",
      "\u001b[31mEpoch: 11, Loss: 0.5764625758016377\u001b[0m\n",
      "\u001b[31mEpoch: 12, Loss: 0.5769134670635511\u001b[0m\n",
      "\u001b[31mEpoch: 13, Loss: 0.574547579686405\u001b[0m\n",
      "\u001b[31mEpoch: 14, Loss: 0.5749269244292479\u001b[0m\n",
      "\u001b[31mEpoch: 15, Loss: 0.574262564540021\u001b[0m\n",
      "\u001b[31mEpoch: 16, Loss: 0.5749779910714216\u001b[0m\n",
      "\u001b[31mEpoch: 17, Loss: 0.5751851896724004\u001b[0m\n",
      "\u001b[31mEpoch: 18, Loss: 0.5743101828330465\u001b[0m\n",
      "\u001b[31mEpoch: 19, Loss: 0.57493859376856\u001b[0m\n",
      "\u001b[31mEpoch: 20, Loss: 0.5736482642339856\u001b[0m\n",
      "\u001b[31mEpoch: 21, Loss: 0.5743087995169538\u001b[0m\n",
      "\u001b[31mEpoch: 22, Loss: 0.5746532149194332\u001b[0m\n",
      "\u001b[31mEpoch: 23, Loss: 0.5734692185363742\u001b[0m\n",
      "\u001b[31mEpoch: 24, Loss: 0.5743303642793095\u001b[0m\n",
      "\u001b[31mEpoch: 25, Loss: 0.5739619679721107\u001b[0m\n",
      "\u001b[31mEpoch: 26, Loss: 0.5739017659191336\u001b[0m\n",
      "\u001b[31mEpoch: 27, Loss: 0.5737657135987326\u001b[0m\n",
      "\u001b[31mEpoch: 28, Loss: 0.5753975102098917\u001b[0m\n",
      "\u001b[31mEpoch: 29, Loss: 0.5732105031386296\u001b[0m\n",
      "\u001b[31mEpoch: 30, Loss: 0.5741491497009434\u001b[0m\n",
      "\u001b[31mEpoch: 31, Loss: 0.5735855608564637\u001b[0m\n",
      "\u001b[31mEpoch: 32, Loss: 0.5732624556911126\u001b[0m\n",
      "\u001b[31mEpoch: 33, Loss: 0.5737534289140157\u001b[0m\n",
      "\u001b[31mEpoch: 34, Loss: 0.573829849563846\u001b[0m\n",
      "\u001b[31mEpoch: 35, Loss: 0.5743476652250754\u001b[0m\n",
      "\u001b[31mEpoch: 36, Loss: 0.5735650501726719\u001b[0m\n",
      "\u001b[31mEpoch: 37, Loss: 0.5732438478549098\u001b[0m\n",
      "\u001b[31mEpoch: 38, Loss: 0.5732627500532271\u001b[0m\n",
      "\u001b[31mEpoch: 39, Loss: 0.5727101544315896\u001b[0m\n",
      "\u001b[31mEpoch: 40, Loss: 0.5732709823308813\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mEpoch: 41, Loss: 0.5724209872636009\u001b[0m\n",
      "\u001b[31mEpoch: 42, Loss: 0.5732695232393143\u001b[0m\n",
      "\u001b[31mEpoch: 43, Loss: 0.5732778837702024\u001b[0m\n",
      "\u001b[31mEpoch: 44, Loss: 0.5730639651640971\u001b[0m\n",
      "\u001b[31mEpoch: 45, Loss: 0.5725613469125403\u001b[0m\n",
      "\u001b[31mEpoch: 46, Loss: 0.5730033754297856\u001b[0m\n",
      "\u001b[31mEpoch: 47, Loss: 0.5724712596766734\u001b[0m\n",
      "\u001b[31mEpoch: 48, Loss: 0.5731576812401247\u001b[0m\n",
      "\u001b[31mEpoch: 49, Loss: 0.5719016253752878\u001b[0m\n",
      "\u001b[31mEpoch: 50, Loss: 0.5727596537813489\u001b[0m\n",
      "\u001b[31mEpoch: 51, Loss: 0.5734839259350345\u001b[0m\n",
      "\u001b[31mEpoch: 52, Loss: 0.5716362460219905\u001b[0m\n",
      "\u001b[31mEpoch: 53, Loss: 0.5726961539618755\u001b[0m\n",
      "\u001b[31mEpoch: 54, Loss: 0.5731491646098064\u001b[0m\n",
      "\u001b[31mEpoch: 55, Loss: 0.5723278553661112\u001b[0m\n",
      "\u001b[31mEpoch: 56, Loss: 0.5728861862186635\u001b[0m\n",
      "\u001b[31mEpoch: 57, Loss: 0.5730129739011495\u001b[0m\n",
      "\u001b[31mEpoch: 58, Loss: 0.5737174994406405\u001b[0m\n",
      "\u001b[31mEpoch: 59, Loss: 0.5730887501259868\u001b[0m\n",
      "\u001b[31mEpoch: 60, Loss: 0.5725076032321105\u001b[0m\n",
      "\u001b[31mEpoch: 61, Loss: 0.5728811696590332\u001b[0m\n",
      "\u001b[31mEpoch: 62, Loss: 0.5727078653481346\u001b[0m\n",
      "\u001b[31mEpoch: 63, Loss: 0.571737149667539\u001b[0m\n",
      "\u001b[31mEpoch: 64, Loss: 0.5724649457393514\u001b[0m\n",
      "\u001b[31mEpoch: 65, Loss: 0.5727239237714573\u001b[0m\n",
      "\u001b[31mEpoch: 66, Loss: 0.5722031320134798\u001b[0m\n",
      "\u001b[31mEpoch: 67, Loss: 0.5721684734361449\u001b[0m\n",
      "\u001b[31mEpoch: 68, Loss: 0.5726188065826\u001b[0m\n",
      "\u001b[31mEpoch: 69, Loss: 0.5715323460654597\u001b[0m\n",
      "\u001b[31mEpoch: 70, Loss: 0.5726311045276762\u001b[0m\n",
      "\u001b[31mEpoch: 71, Loss: 0.5721848855276456\u001b[0m\n",
      "\u001b[31mEpoch: 72, Loss: 0.5713600274663739\u001b[0m\n",
      "\u001b[31mEpoch: 73, Loss: 0.5716521214903071\u001b[0m\n",
      "\u001b[31mEpoch: 74, Loss: 0.573741498143039\u001b[0m\n",
      "\u001b[31mEpoch: 75, Loss: 0.5721493475669333\u001b[0m\n",
      "\u001b[31mEpoch: 76, Loss: 0.5713349816002203\u001b[0m\n",
      "\u001b[31mEpoch: 77, Loss: 0.5718255986351423\u001b[0m\n",
      "\u001b[31mEpoch: 78, Loss: 0.5717339948210377\u001b[0m\n",
      "\u001b[31mEpoch: 79, Loss: 0.5717365151822343\u001b[0m\n",
      "\u001b[31mEpoch: 80, Loss: 0.5716478792078487\u001b[0m\n",
      "\u001b[31mEpoch: 81, Loss: 0.5718116213515233\u001b[0m\n",
      "\u001b[31mEpoch: 82, Loss: 0.5713134848558055\u001b[0m\n",
      "\u001b[31mEpoch: 83, Loss: 0.5724054723810614\u001b[0m\n",
      "\u001b[31mEpoch: 84, Loss: 0.5725094739752316\u001b[0m\n",
      "\u001b[31mEpoch: 85, Loss: 0.5728796849397014\u001b[0m\n",
      "\u001b[31mEpoch: 86, Loss: 0.5728912454056606\u001b[0m\n",
      "\u001b[31mEpoch: 87, Loss: 0.5716046263299854\u001b[0m\n",
      "\u001b[31mEpoch: 88, Loss: 0.571843955305855\u001b[0m\n",
      "\u001b[31mEpoch: 89, Loss: 0.5715076931658086\u001b[0m\n",
      "\u001b[31mEpoch: 90, Loss: 0.5729681545624349\u001b[0m\n",
      "\u001b[31mEpoch: 91, Loss: 0.5721486964196748\u001b[0m\n",
      "\u001b[31mEpoch: 92, Loss: 0.5716133079557829\u001b[0m\n",
      "\u001b[31mEpoch: 93, Loss: 0.5719925092205349\u001b[0m\n",
      "\u001b[31mEpoch: 94, Loss: 0.5726121164300245\u001b[0m\n",
      "\u001b[31mEpoch: 95, Loss: 0.5716464078018951\u001b[0m\n",
      "\u001b[31mEpoch: 96, Loss: 0.5714714060868886\u001b[0m\n",
      "\u001b[31mEpoch: 97, Loss: 0.5720177567295367\u001b[0m\n",
      "\u001b[31mEpoch: 98, Loss: 0.5711014238608241\u001b[0m\n",
      "\u001b[31mEpoch: 99, Loss: 0.5701817607081562\u001b[0m\n",
      "\u001b[31mEpoch: 100, Loss: 0.5717458461917072\u001b[0m\n",
      "\u001b[31m2019-06-28 21:20:20,184 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2019-06-28 21:21:14 Uploading - Uploading generated training model\n",
      "2019-06-28 21:21:14 Completed - Training job completed\n",
      "Billable seconds: 986\n"
     ]
    }
   ],
   "source": [
    "# specify an output path\n",
    "output_path = f\"s3://{bucket}/{prefix}-lower-hidden-dim\"\n",
    "\n",
    "# instantiate a pytorch estimator\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"pytorch\",\n",
    "    role=role,\n",
    "    framework_version=\"1.0\",\n",
    "    train_instance_count=1,\n",
    "    train_instance_type=\"ml.p2.xlarge\", # \"ml.c4.xlarge\",\n",
    "    output_path=output_path,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    hyperparameters={\n",
    "        \"input_features\": 19,\n",
    "        \"hidden_dim\": 15, \n",
    "        \"output_dim\": 1,\n",
    "        \"epochs\": 100\n",
    "    })\n",
    "train_data_path = 's3://sagemaker-us-west-2-203336335427/starbucks-xgboost/train.csv'\n",
    "estimator.fit({'train': train_data_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "# Deploy the trained model\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "model = PyTorchModel(\n",
    "    entry_point=\"predict.py\",\n",
    "    role=role, \n",
    "    framework_version=\"1.0\",\n",
    "    model_data=estimator.model_data,\n",
    "    source_dir=\"pytorch\"\n",
    ")\n",
    "\n",
    "# deploy your model to create a predictor\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type=\"ml.t2.medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating the model\n",
    "# read in test data, assuming it is stored locally\n",
    "test_data = pd.read_csv(\"data/test_full.csv\", header=None, names=None)\n",
    "\n",
    "# labels are in the first column\n",
    "test_y = test_data.iloc[:,0]\n",
    "test_x = test_data.iloc[:,1:]\n",
    "test_y_preds = np.hstack(\n",
    "    predictor.predict(test_x.iloc[i: i+1000]).reshape(1, -1).squeeze() \n",
    "    for i in range(0, len(test_x), 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7009638194081423"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the test roc_auc_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(test_y, test_y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7056895647614053"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_y, test_y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higher epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-28 22:19:15 Starting - Starting the training job...\n",
      "2019-06-28 22:19:17 Starting - Launching requested ML instances...\n",
      "2019-06-28 22:20:15 Starting - Preparing the instances for training......\n",
      "2019-06-28 22:21:08 Downloading - Downloading input data...\n",
      "2019-06-28 22:21:41 Training - Training image download completed. Training in progress..\n",
      "\u001b[31mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[31mbash: no job control in this shell\u001b[0m\n",
      "\u001b[31m2019-06-28 22:21:42,549 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[31m2019-06-28 22:21:42,551 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[31m2019-06-28 22:21:42,563 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[31m2019-06-28 22:21:42,564 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[31m2019-06-28 22:21:42,869 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[31mGenerating setup.py\u001b[0m\n",
      "\u001b[31m2019-06-28 22:21:42,869 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[31m2019-06-28 22:21:42,869 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[31m2019-06-28 22:21:42,869 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[31m/usr/bin/python -m pip install -U . \u001b[0m\n",
      "\u001b[31mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[31mBuilding wheels for collected packages: train\n",
      "  Running setup.py bdist_wheel for train: started\n",
      "  Running setup.py bdist_wheel for train: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-glxmx5gl/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[31mSuccessfully built train\u001b[0m\n",
      "\u001b[31mInstalling collected packages: train\u001b[0m\n",
      "\u001b[31mSuccessfully installed train-1.0.0\u001b[0m\n",
      "\u001b[31mYou are using pip version 18.1, however version 19.1.1 is available.\u001b[0m\n",
      "\u001b[31mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[31m2019-06-28 22:21:44,452 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[31m2019-06-28 22:21:44,464 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[31mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[31m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"hidden_dim\": 15,\n",
      "        \"input_features\": 19,\n",
      "        \"epochs\": 200,\n",
      "        \"output_dim\": 1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-pytorch-2019-06-28-22-19-15-412\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-203336335427/sagemaker-pytorch-2019-06-28-22-19-15-412/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[31m}\n",
      "\u001b[0m\n",
      "\u001b[31mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[31mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[31mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[31mSM_HPS={\"epochs\":200,\"hidden_dim\":15,\"input_features\":19,\"output_dim\":1}\u001b[0m\n",
      "\u001b[31mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[31mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[31mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[31mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[31mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[31mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[31mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[31mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[31mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[31mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[31mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[31mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[31mSM_MODULE_DIR=s3://sagemaker-us-west-2-203336335427/sagemaker-pytorch-2019-06-28-22-19-15-412/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[31mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":200,\"hidden_dim\":15,\"input_features\":19,\"output_dim\":1},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-pytorch-2019-06-28-22-19-15-412\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-203336335427/sagemaker-pytorch-2019-06-28-22-19-15-412/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[31mSM_USER_ARGS=[\"--epochs\",\"200\",\"--hidden_dim\",\"15\",\"--input_features\",\"19\",\"--output_dim\",\"1\"]\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[31mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[31mSM_HP_HIDDEN_DIM=15\u001b[0m\n",
      "\u001b[31mSM_HP_INPUT_FEATURES=19\u001b[0m\n",
      "\u001b[31mSM_HP_EPOCHS=200\u001b[0m\n",
      "\u001b[31mSM_HP_OUTPUT_DIM=1\u001b[0m\n",
      "\u001b[31mPYTHONPATH=/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[0m\n",
      "\u001b[31mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[31m/usr/bin/python -m train --epochs 200 --hidden_dim 15 --input_features 19 --output_dim 1\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[31mUsing device cpu.\u001b[0m\n",
      "\u001b[31mGet train data loader.\u001b[0m\n",
      "\u001b[31mEpoch: 1, Loss: 0.609162207706814\u001b[0m\n",
      "\u001b[31mEpoch: 2, Loss: 0.5917049872350604\u001b[0m\n",
      "\u001b[31mEpoch: 3, Loss: 0.5885872362397806\u001b[0m\n",
      "\u001b[31mEpoch: 4, Loss: 0.5858726387445846\u001b[0m\n",
      "\u001b[31mEpoch: 5, Loss: 0.5822181419654285\u001b[0m\n",
      "\u001b[31mEpoch: 6, Loss: 0.5799668145341597\u001b[0m\n",
      "\u001b[31mEpoch: 7, Loss: 0.5791358395657512\u001b[0m\n",
      "\u001b[31mEpoch: 8, Loss: 0.5777142768746681\u001b[0m\n",
      "\u001b[31mEpoch: 9, Loss: 0.5768584179716387\u001b[0m\n",
      "\u001b[31mEpoch: 10, Loss: 0.5772881651220474\u001b[0m\n",
      "\u001b[31mEpoch: 11, Loss: 0.5764988605365547\u001b[0m\n",
      "\u001b[31mEpoch: 12, Loss: 0.5744088184186136\u001b[0m\n",
      "\u001b[31mEpoch: 13, Loss: 0.5752970731716031\u001b[0m\n",
      "\u001b[31mEpoch: 14, Loss: 0.5744708651990703\u001b[0m\n",
      "\u001b[31mEpoch: 15, Loss: 0.5755991889854496\u001b[0m\n",
      "\u001b[31mEpoch: 16, Loss: 0.5748910464513838\u001b[0m\n",
      "\u001b[31mEpoch: 17, Loss: 0.5751269347435526\u001b[0m\n",
      "\u001b[31mEpoch: 18, Loss: 0.5751364955880892\u001b[0m\n",
      "\u001b[31mEpoch: 19, Loss: 0.5744202934875248\u001b[0m\n",
      "\u001b[31mEpoch: 20, Loss: 0.5740863729059027\u001b[0m\n",
      "\u001b[31mEpoch: 21, Loss: 0.5744332513670797\u001b[0m\n",
      "\u001b[31mEpoch: 22, Loss: 0.5763389916455701\u001b[0m\n",
      "\u001b[31mEpoch: 23, Loss: 0.57366527356905\u001b[0m\n",
      "\u001b[31mEpoch: 24, Loss: 0.5739050593631767\u001b[0m\n",
      "\u001b[31mEpoch: 25, Loss: 0.5737719905398758\u001b[0m\n",
      "\u001b[31mEpoch: 26, Loss: 0.5736317843924301\u001b[0m\n",
      "\u001b[31mEpoch: 27, Loss: 0.573790728493353\u001b[0m\n",
      "\u001b[31mEpoch: 28, Loss: 0.5735605954230948\u001b[0m\n",
      "\u001b[31mEpoch: 29, Loss: 0.5733179919031006\u001b[0m\n",
      "\u001b[31mEpoch: 30, Loss: 0.5732744138841326\u001b[0m\n",
      "\u001b[31mEpoch: 31, Loss: 0.5743125348780932\u001b[0m\n",
      "\u001b[31mEpoch: 32, Loss: 0.5739484060123172\u001b[0m\n",
      "\u001b[31mEpoch: 33, Loss: 0.5729487802456604\u001b[0m\n",
      "\u001b[31mEpoch: 34, Loss: 0.5744153738831089\u001b[0m\n",
      "\u001b[31mEpoch: 35, Loss: 0.5733263583665483\u001b[0m\n",
      "\u001b[31mEpoch: 36, Loss: 0.5728099160426565\u001b[0m\n",
      "\u001b[31mEpoch: 37, Loss: 0.5732303272528149\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mEpoch: 38, Loss: 0.5735095308458761\u001b[0m\n",
      "\u001b[31mEpoch: 39, Loss: 0.5728003907013922\u001b[0m\n",
      "\u001b[31mEpoch: 40, Loss: 0.5736741857945026\u001b[0m\n",
      "\u001b[31mEpoch: 41, Loss: 0.5736510265269753\u001b[0m\n",
      "\u001b[31mEpoch: 42, Loss: 0.5721236345863967\u001b[0m\n",
      "\u001b[31mEpoch: 43, Loss: 0.5746038772472728\u001b[0m\n",
      "\u001b[31mEpoch: 44, Loss: 0.572900599615181\u001b[0m\n",
      "\u001b[31mEpoch: 45, Loss: 0.5730355562956145\u001b[0m\n",
      "\u001b[31mEpoch: 46, Loss: 0.5732300898806656\u001b[0m\n",
      "\u001b[31mEpoch: 47, Loss: 0.5730788228245041\u001b[0m\n",
      "\u001b[31mEpoch: 48, Loss: 0.5728198857147818\u001b[0m\n",
      "\u001b[31mEpoch: 49, Loss: 0.571591656328587\u001b[0m\n",
      "\u001b[31mEpoch: 50, Loss: 0.5722077148293288\u001b[0m\n",
      "\u001b[31mEpoch: 51, Loss: 0.5728044840001435\u001b[0m\n",
      "\u001b[31mEpoch: 52, Loss: 0.5724969982124223\u001b[0m\n",
      "\u001b[31mEpoch: 53, Loss: 0.5733874418036768\u001b[0m\n",
      "\u001b[31mEpoch: 54, Loss: 0.5727589353277219\u001b[0m\n",
      "\u001b[31mEpoch: 55, Loss: 0.5716134936761543\u001b[0m\n",
      "\u001b[31mEpoch: 56, Loss: 0.5738802883732185\u001b[0m\n",
      "\u001b[31mEpoch: 57, Loss: 0.5709626779071847\u001b[0m\n",
      "\u001b[31mEpoch: 58, Loss: 0.572723510629787\u001b[0m\n",
      "\u001b[31mEpoch: 59, Loss: 0.5728201337828395\u001b[0m\n",
      "\u001b[31mEpoch: 60, Loss: 0.5720391281283973\u001b[0m\n",
      "\u001b[31mEpoch: 61, Loss: 0.5725958760506875\u001b[0m\n",
      "\u001b[31mEpoch: 62, Loss: 0.5708284554437975\u001b[0m\n",
      "\u001b[31mEpoch: 63, Loss: 0.5728359947295001\u001b[0m\n",
      "\u001b[31mEpoch: 64, Loss: 0.5715046634946424\u001b[0m\n",
      "\u001b[31mEpoch: 65, Loss: 0.5716282190315509\u001b[0m\n",
      "\u001b[31mEpoch: 66, Loss: 0.5715499758859922\u001b[0m\n",
      "\u001b[31mEpoch: 67, Loss: 0.5728800091794815\u001b[0m\n",
      "\u001b[31mEpoch: 68, Loss: 0.5723741822223761\u001b[0m\n",
      "\u001b[31mEpoch: 69, Loss: 0.5721300419127003\u001b[0m\n",
      "\u001b[31mEpoch: 70, Loss: 0.5722914776440416\u001b[0m\n",
      "\u001b[31mEpoch: 71, Loss: 0.5738472132395939\u001b[0m\n",
      "\u001b[31mEpoch: 72, Loss: 0.5739355845844255\u001b[0m\n",
      "\u001b[31mEpoch: 73, Loss: 0.572587960732452\u001b[0m\n",
      "\u001b[31mEpoch: 74, Loss: 0.5718807885848851\u001b[0m\n",
      "\u001b[31mEpoch: 75, Loss: 0.5728475693097052\u001b[0m\n",
      "\u001b[31mEpoch: 76, Loss: 0.5725537027312575\u001b[0m\n",
      "\u001b[31mEpoch: 77, Loss: 0.572778752803356\u001b[0m\n",
      "\u001b[31mEpoch: 78, Loss: 0.5706565337430002\u001b[0m\n",
      "\u001b[31mEpoch: 79, Loss: 0.5723513474206576\u001b[0m\n",
      "\u001b[31mEpoch: 80, Loss: 0.5716113309009692\u001b[0m\n",
      "\u001b[31mEpoch: 81, Loss: 0.571866068986248\u001b[0m\n",
      "\u001b[31mEpoch: 82, Loss: 0.5714335282169255\u001b[0m\n",
      "\u001b[31mEpoch: 83, Loss: 0.5704078761407275\u001b[0m\n",
      "\u001b[31mEpoch: 84, Loss: 0.5718679401814268\u001b[0m\n",
      "\u001b[31mEpoch: 85, Loss: 0.5700533584541596\u001b[0m\n",
      "\u001b[31mEpoch: 86, Loss: 0.5714845993322379\u001b[0m\n",
      "\u001b[31mEpoch: 87, Loss: 0.5717084798417734\u001b[0m\n",
      "\u001b[31mEpoch: 88, Loss: 0.5712051370226489\u001b[0m\n",
      "\u001b[31mEpoch: 89, Loss: 0.5725427880297216\u001b[0m\n",
      "\u001b[31mEpoch: 90, Loss: 0.5718817526425761\u001b[0m\n",
      "\u001b[31mEpoch: 91, Loss: 0.5728354944913798\u001b[0m\n",
      "\u001b[31mEpoch: 92, Loss: 0.5711548540894682\u001b[0m\n",
      "\u001b[31mEpoch: 93, Loss: 0.5726218345264594\u001b[0m\n",
      "\u001b[31mEpoch: 94, Loss: 0.5724823153978876\u001b[0m\n",
      "\u001b[31mEpoch: 95, Loss: 0.5715022864915459\u001b[0m\n",
      "\u001b[31mEpoch: 96, Loss: 0.5717674897181407\u001b[0m\n",
      "\u001b[31mEpoch: 97, Loss: 0.5722980345791199\u001b[0m\n",
      "\u001b[31mEpoch: 98, Loss: 0.5711579181477149\u001b[0m\n",
      "\u001b[31mEpoch: 99, Loss: 0.5706704615988535\u001b[0m\n",
      "\u001b[31mEpoch: 100, Loss: 0.5716419970470198\u001b[0m\n",
      "\u001b[31mEpoch: 101, Loss: 0.5708555047654927\u001b[0m\n",
      "\u001b[31mEpoch: 102, Loss: 0.57211297535048\u001b[0m\n",
      "\u001b[31mEpoch: 103, Loss: 0.5726173603104965\u001b[0m\n",
      "\u001b[31mEpoch: 104, Loss: 0.571097085862124\u001b[0m\n",
      "\u001b[31mEpoch: 105, Loss: 0.5720192869643816\u001b[0m\n",
      "\u001b[31mEpoch: 106, Loss: 0.5711964545457551\u001b[0m\n",
      "\u001b[31mEpoch: 107, Loss: 0.5724087786082918\u001b[0m\n",
      "\u001b[31mEpoch: 108, Loss: 0.571474882759405\u001b[0m\n",
      "\u001b[31mEpoch: 109, Loss: 0.5706173906779468\u001b[0m\n",
      "\u001b[31mEpoch: 110, Loss: 0.5732623682598049\u001b[0m\n",
      "\u001b[31mEpoch: 111, Loss: 0.570721660216761\u001b[0m\n",
      "\u001b[31mEpoch: 112, Loss: 0.5711459904574276\u001b[0m\n",
      "\u001b[31mEpoch: 113, Loss: 0.5724428551250629\u001b[0m\n",
      "\u001b[31mEpoch: 114, Loss: 0.5723638563232029\u001b[0m\n",
      "\u001b[31mEpoch: 115, Loss: 0.5706354141067923\u001b[0m\n",
      "\u001b[31mEpoch: 116, Loss: 0.5716926460297367\u001b[0m\n",
      "\u001b[31mEpoch: 117, Loss: 0.5718424808220024\u001b[0m\n",
      "\u001b[31mEpoch: 118, Loss: 0.5722026521821371\u001b[0m\n",
      "\u001b[31mEpoch: 119, Loss: 0.5711415846584441\u001b[0m\n",
      "\u001b[31mEpoch: 120, Loss: 0.5722495718060361\u001b[0m\n",
      "\u001b[31mEpoch: 121, Loss: 0.5722236954261748\u001b[0m\n",
      "\u001b[31mEpoch: 122, Loss: 0.5713300110918752\u001b[0m\n",
      "\u001b[31mEpoch: 123, Loss: 0.5720591136634573\u001b[0m\n",
      "\u001b[31mEpoch: 124, Loss: 0.5708486657408293\u001b[0m\n",
      "\u001b[31mEpoch: 125, Loss: 0.572213032275215\u001b[0m\n",
      "\u001b[31mEpoch: 126, Loss: 0.5703306850031967\u001b[0m\n",
      "\u001b[31mEpoch: 127, Loss: 0.5726474094256926\u001b[0m\n",
      "\u001b[31mEpoch: 128, Loss: 0.5712877459963609\u001b[0m\n",
      "\u001b[31mEpoch: 129, Loss: 0.5716114567404382\u001b[0m\n",
      "\u001b[31mEpoch: 130, Loss: 0.5721633745880609\u001b[0m\n",
      "\u001b[31mEpoch: 131, Loss: 0.5722608518678597\u001b[0m\n",
      "\u001b[31mEpoch: 132, Loss: 0.5712872261718641\u001b[0m\n",
      "\u001b[31mEpoch: 133, Loss: 0.5714379511020157\u001b[0m\n",
      "\u001b[31mEpoch: 134, Loss: 0.5712309957080789\u001b[0m\n",
      "\u001b[31mEpoch: 135, Loss: 0.5719073807171892\u001b[0m\n",
      "\u001b[31mEpoch: 136, Loss: 0.5721886885579159\u001b[0m\n",
      "\u001b[31mEpoch: 137, Loss: 0.5703604212888841\u001b[0m\n",
      "\u001b[31mEpoch: 138, Loss: 0.5720694075139721\u001b[0m\n",
      "\u001b[31mEpoch: 139, Loss: 0.5710992984836467\u001b[0m\n",
      "\u001b[31mEpoch: 140, Loss: 0.5708216797825549\u001b[0m\n",
      "\u001b[31mEpoch: 141, Loss: 0.5701254364507699\u001b[0m\n",
      "\u001b[31mEpoch: 142, Loss: 0.571688367895196\u001b[0m\n",
      "\u001b[31mEpoch: 143, Loss: 0.5715770669393102\u001b[0m\n",
      "\u001b[31mEpoch: 144, Loss: 0.5697002208941662\u001b[0m\n",
      "\u001b[31mEpoch: 145, Loss: 0.5714216910553782\u001b[0m\n",
      "\u001b[31mEpoch: 146, Loss: 0.5707942468871114\u001b[0m\n",
      "\u001b[31mEpoch: 147, Loss: 0.572182132532534\u001b[0m\n",
      "\u001b[31mEpoch: 148, Loss: 0.5720181932275215\u001b[0m\n",
      "\u001b[31mEpoch: 149, Loss: 0.5713440767388219\u001b[0m\n",
      "\u001b[31mEpoch: 150, Loss: 0.5711833419536383\u001b[0m\n",
      "\u001b[31mEpoch: 151, Loss: 0.5714423420556476\u001b[0m\n",
      "\u001b[31mEpoch: 152, Loss: 0.5718244563941652\u001b[0m\n",
      "\u001b[31mEpoch: 153, Loss: 0.5714026936849628\u001b[0m\n",
      "\u001b[31mEpoch: 154, Loss: 0.5717111616712831\u001b[0m\n",
      "\u001b[31mEpoch: 155, Loss: 0.5710676648280817\u001b[0m\n",
      "\u001b[31mEpoch: 156, Loss: 0.571742213986228\u001b[0m\n",
      "\u001b[31mEpoch: 157, Loss: 0.5721984863448679\u001b[0m\n",
      "\u001b[31mEpoch: 158, Loss: 0.5710431498144003\u001b[0m\n",
      "\u001b[31mEpoch: 159, Loss: 0.5716277375789394\u001b[0m\n",
      "\u001b[31mEpoch: 160, Loss: 0.5700351844259193\u001b[0m\n",
      "\u001b[31mEpoch: 161, Loss: 0.5715097754133328\u001b[0m\n",
      "\u001b[31mEpoch: 162, Loss: 0.5709240106551835\u001b[0m\n",
      "\u001b[31mEpoch: 163, Loss: 0.5711160015189246\u001b[0m\n",
      "\u001b[31mEpoch: 164, Loss: 0.5710899502345909\u001b[0m\n",
      "\u001b[31mEpoch: 165, Loss: 0.5719348920334815\u001b[0m\n",
      "\u001b[31mEpoch: 166, Loss: 0.5717170943174693\u001b[0m\n",
      "\u001b[31mEpoch: 167, Loss: 0.5719859162882696\u001b[0m\n",
      "\u001b[31mEpoch: 168, Loss: 0.571300456318516\u001b[0m\n",
      "\u001b[31mEpoch: 169, Loss: 0.5715222116155124\u001b[0m\n",
      "\u001b[31mEpoch: 170, Loss: 0.5708268391114942\u001b[0m\n",
      "\u001b[31mEpoch: 171, Loss: 0.571053041456344\u001b[0m\n",
      "\u001b[31mEpoch: 172, Loss: 0.572257792952467\u001b[0m\n",
      "\u001b[31mEpoch: 173, Loss: 0.5717340305001102\u001b[0m\n",
      "\u001b[31mEpoch: 174, Loss: 0.5711925425528364\u001b[0m\n",
      "\u001b[31mEpoch: 175, Loss: 0.5711841517591968\u001b[0m\n",
      "\u001b[31mEpoch: 176, Loss: 0.5734628799563267\u001b[0m\n",
      "\u001b[31mEpoch: 177, Loss: 0.5707001571547002\u001b[0m\n",
      "\u001b[31mEpoch: 178, Loss: 0.570298352491543\u001b[0m\n",
      "\u001b[31mEpoch: 179, Loss: 0.5703593428884999\u001b[0m\n",
      "\u001b[31mEpoch: 180, Loss: 0.5709994683384002\u001b[0m\n",
      "\u001b[31mEpoch: 181, Loss: 0.5712329138168012\u001b[0m\n",
      "\u001b[31mEpoch: 182, Loss: 0.5707732709271185\u001b[0m\n",
      "\u001b[31mEpoch: 183, Loss: 0.571286139341441\u001b[0m\n",
      "\u001b[31mEpoch: 184, Loss: 0.5722275134329492\u001b[0m\n",
      "\u001b[31mEpoch: 185, Loss: 0.5713232271931368\u001b[0m\n",
      "\u001b[31mEpoch: 186, Loss: 0.5726235321482246\u001b[0m\n",
      "\u001b[31mEpoch: 187, Loss: 0.5710601350704159\u001b[0m\n",
      "\u001b[31mEpoch: 188, Loss: 0.5712598319050302\u001b[0m\n",
      "\u001b[31mEpoch: 189, Loss: 0.570651151652863\u001b[0m\n",
      "\u001b[31mEpoch: 190, Loss: 0.5699590527469969\u001b[0m\n",
      "\u001b[31mEpoch: 191, Loss: 0.5705318155862419\u001b[0m\n",
      "\u001b[31mEpoch: 192, Loss: 0.5715607472657042\u001b[0m\n",
      "\u001b[31mEpoch: 193, Loss: 0.5707174383215466\u001b[0m\n",
      "\u001b[31mEpoch: 194, Loss: 0.5707485687587591\u001b[0m\n",
      "\u001b[31mEpoch: 195, Loss: 0.5705690648839268\u001b[0m\n",
      "\u001b[31mEpoch: 196, Loss: 0.5707067698435614\u001b[0m\n",
      "\u001b[31mEpoch: 197, Loss: 0.5709123884973009\u001b[0m\n",
      "\n",
      "2019-06-28 22:33:22 Uploading - Uploading generated training model\u001b[31mEpoch: 198, Loss: 0.5709623576745335\u001b[0m\n",
      "\u001b[31mEpoch: 199, Loss: 0.5715733385711127\u001b[0m\n",
      "\u001b[31mEpoch: 200, Loss: 0.5708520480560676\u001b[0m\n",
      "\u001b[31m2019-06-28 22:33:19,246 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2019-06-28 22:33:27 Completed - Training job completed\n",
      "Billable seconds: 740\n"
     ]
    }
   ],
   "source": [
    "# import a PyTorch wrapper\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# specify an output path\n",
    "output_path = f\"s3://{bucket}/{prefix}-higher-epochs\"\n",
    "\n",
    "# instantiate a pytorch estimator\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"pytorch\",\n",
    "    role=role,\n",
    "    framework_version=\"1.0\",\n",
    "    train_instance_count=1,\n",
    "    train_instance_type=\"ml.c4.xlarge\",\n",
    "    output_path=output_path,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    hyperparameters={\n",
    "        \"input_features\": 19,\n",
    "        \"hidden_dim\": 15, \n",
    "        \"output_dim\": 1,\n",
    "        \"epochs\": 200\n",
    "    })\n",
    "train_data_path = 's3://sagemaker-us-west-2-203336335427/starbucks-xgboost/train.csv'\n",
    "estimator.fit({'train': train_data_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "# Deploy the trained model\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "model = PyTorchModel(\n",
    "    entry_point=\"predict.py\",\n",
    "    role=role, \n",
    "    framework_version=\"1.0\",\n",
    "    model_data=estimator.model_data,\n",
    "    source_dir=\"pytorch\"\n",
    ")\n",
    "\n",
    "# deploy your model to create a predictor\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type=\"ml.t2.medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating the model\n",
    "# read in test data, assuming it is stored locally\n",
    "test_data = pd.read_csv(\"data/test_full.csv\", header=None, names=None)\n",
    "\n",
    "# labels are in the first column\n",
    "test_y = test_data.iloc[:,0]\n",
    "test_x = test_data.iloc[:,1:]\n",
    "test_y_preds = np.hstack(\n",
    "    predictor.predict(test_x.iloc[i: i+1000]).reshape(1, -1).squeeze() \n",
    "    for i in range(0, len(test_x), 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7111692582105572"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the test roc_auc_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(test_y, test_y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7176560041950708"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_y, test_y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5x hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-28 22:46:13 Starting - Starting the training job...\n",
      "2019-06-28 22:46:14 Starting - Launching requested ML instances...\n",
      "2019-06-28 22:47:11 Starting - Preparing the instances for training......\n",
      "2019-06-28 22:48:05 Downloading - Downloading input data...\n",
      "2019-06-28 22:48:26 Training - Downloading the training image.\n",
      "\u001b[31mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[31mbash: no job control in this shell\u001b[0m\n",
      "\u001b[31m2019-06-28 22:48:40,214 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[31m2019-06-28 22:48:40,217 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[31m2019-06-28 22:48:40,230 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[31m2019-06-28 22:48:43,250 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[31m2019-06-28 22:48:43,523 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[31mGenerating setup.py\u001b[0m\n",
      "\u001b[31m2019-06-28 22:48:43,523 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[31m2019-06-28 22:48:43,523 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[31m2019-06-28 22:48:43,523 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[31m/usr/bin/python -m pip install -U . \u001b[0m\n",
      "\u001b[31mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[31mBuilding wheels for collected packages: train\n",
      "  Running setup.py bdist_wheel for train: started\u001b[0m\n",
      "\u001b[31m  Running setup.py bdist_wheel for train: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-lo1prdj1/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[31mSuccessfully built train\u001b[0m\n",
      "\u001b[31mInstalling collected packages: train\u001b[0m\n",
      "\u001b[31mSuccessfully installed train-1.0.0\u001b[0m\n",
      "\u001b[31mYou are using pip version 18.1, however version 19.1.1 is available.\u001b[0m\n",
      "\u001b[31mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[31m2019-06-28 22:48:45,103 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[31m2019-06-28 22:48:45,115 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[31mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[31m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"hidden_dim\": 100,\n",
      "        \"input_features\": 19,\n",
      "        \"epochs\": 100,\n",
      "        \"output_dim\": 1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-pytorch-2019-06-28-22-46-12-780\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-203336335427/sagemaker-pytorch-2019-06-28-22-46-12-780/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[31m}\n",
      "\u001b[0m\n",
      "\u001b[31mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[31mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[31mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[31mSM_HPS={\"epochs\":100,\"hidden_dim\":100,\"input_features\":19,\"output_dim\":1}\u001b[0m\n",
      "\u001b[31mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[31mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[31mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[31mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[31mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[31mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[31mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[31mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[31mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[31mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[31mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[31mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[31mSM_MODULE_DIR=s3://sagemaker-us-west-2-203336335427/sagemaker-pytorch-2019-06-28-22-46-12-780/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[31mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":100,\"hidden_dim\":100,\"input_features\":19,\"output_dim\":1},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-pytorch-2019-06-28-22-46-12-780\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-203336335427/sagemaker-pytorch-2019-06-28-22-46-12-780/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[31mSM_USER_ARGS=[\"--epochs\",\"100\",\"--hidden_dim\",\"100\",\"--input_features\",\"19\",\"--output_dim\",\"1\"]\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[31mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[31mSM_HP_HIDDEN_DIM=100\u001b[0m\n",
      "\u001b[31mSM_HP_INPUT_FEATURES=19\u001b[0m\n",
      "\u001b[31mSM_HP_EPOCHS=100\u001b[0m\n",
      "\u001b[31mSM_HP_OUTPUT_DIM=1\u001b[0m\n",
      "\u001b[31mPYTHONPATH=/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[0m\n",
      "\u001b[31mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[31m/usr/bin/python -m train --epochs 100 --hidden_dim 100 --input_features 19 --output_dim 1\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[31mUsing device cpu.\u001b[0m\n",
      "\u001b[31mGet train data loader.\u001b[0m\n",
      "\u001b[31mEpoch: 1, Loss: 0.5947728141556295\u001b[0m\n",
      "\u001b[31mEpoch: 2, Loss: 0.577398305916496\u001b[0m\n",
      "\u001b[31mEpoch: 3, Loss: 0.5697719692932756\u001b[0m\n",
      "\u001b[31mEpoch: 4, Loss: 0.5663139430045635\u001b[0m\n",
      "\n",
      "2019-06-28 22:48:47 Training - Training image download completed. Training in progress.\u001b[31mEpoch: 5, Loss: 0.5640031185210421\u001b[0m\n",
      "\u001b[31mEpoch: 6, Loss: 0.5640671021175965\u001b[0m\n",
      "\u001b[31mEpoch: 7, Loss: 0.5630325311941378\u001b[0m\n",
      "\u001b[31mEpoch: 8, Loss: 0.56187896498022\u001b[0m\n",
      "\u001b[31mEpoch: 9, Loss: 0.5628136351286026\u001b[0m\n",
      "\u001b[31mEpoch: 10, Loss: 0.5609488416030612\u001b[0m\n",
      "\u001b[31mEpoch: 11, Loss: 0.560789579002375\u001b[0m\n",
      "\u001b[31mEpoch: 12, Loss: 0.5604464253068864\u001b[0m\n",
      "\u001b[31mEpoch: 13, Loss: 0.5610801890827297\u001b[0m\n",
      "\u001b[31mEpoch: 14, Loss: 0.559871499912904\u001b[0m\n",
      "\u001b[31mEpoch: 15, Loss: 0.5600907464766324\u001b[0m\n",
      "\u001b[31mEpoch: 16, Loss: 0.5599657961212741\u001b[0m\n",
      "\u001b[31mEpoch: 17, Loss: 0.5600197544984157\u001b[0m\n",
      "\u001b[31mEpoch: 18, Loss: 0.5591202019026663\u001b[0m\n",
      "\u001b[31mEpoch: 19, Loss: 0.5584946673768066\u001b[0m\n",
      "\u001b[31mEpoch: 20, Loss: 0.5592142882334828\u001b[0m\n",
      "\u001b[31mEpoch: 21, Loss: 0.5592829408996114\u001b[0m\n",
      "\u001b[31mEpoch: 22, Loss: 0.5580728661729378\u001b[0m\n",
      "\u001b[31mEpoch: 23, Loss: 0.5582966538097305\u001b[0m\n",
      "\u001b[31mEpoch: 24, Loss: 0.5586281279890278\u001b[0m\n",
      "\u001b[31mEpoch: 25, Loss: 0.5588579197250279\u001b[0m\n",
      "\u001b[31mEpoch: 26, Loss: 0.5577441387045919\u001b[0m\n",
      "\u001b[31mEpoch: 27, Loss: 0.5574731354995837\u001b[0m\n",
      "\u001b[31mEpoch: 28, Loss: 0.5579454260400395\u001b[0m\n",
      "\u001b[31mEpoch: 29, Loss: 0.5578871909487113\u001b[0m\n",
      "\u001b[31mEpoch: 30, Loss: 0.5475844799841612\u001b[0m\n",
      "\u001b[31mEpoch: 31, Loss: 0.548051632660047\u001b[0m\n",
      "\u001b[31mEpoch: 32, Loss: 0.5463213903024402\u001b[0m\n",
      "\u001b[31mEpoch: 33, Loss: 0.5483657537925333\u001b[0m\n",
      "\u001b[31mEpoch: 34, Loss: 0.5473804865810978\u001b[0m\n",
      "\u001b[31mEpoch: 35, Loss: 0.5471866181040748\u001b[0m\n",
      "\u001b[31mEpoch: 36, Loss: 0.5470517874054248\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mEpoch: 37, Loss: 0.5468553893028128\u001b[0m\n",
      "\u001b[31mEpoch: 38, Loss: 0.5373431954373804\u001b[0m\n",
      "\u001b[31mEpoch: 39, Loss: 0.5367521486556932\u001b[0m\n",
      "\u001b[31mEpoch: 40, Loss: 0.5365186577231697\u001b[0m\n",
      "\u001b[31mEpoch: 41, Loss: 0.5375246298078741\u001b[0m\n",
      "\u001b[31mEpoch: 42, Loss: 0.5360836938464463\u001b[0m\n",
      "\u001b[31mEpoch: 43, Loss: 0.536555456659767\u001b[0m\n",
      "\u001b[31mEpoch: 44, Loss: 0.5264785062066848\u001b[0m\n",
      "\u001b[31mEpoch: 45, Loss: 0.5262435746806837\u001b[0m\n",
      "\u001b[31mEpoch: 46, Loss: 0.5269687865562877\u001b[0m\n",
      "\u001b[31mEpoch: 47, Loss: 0.5264477149355278\u001b[0m\n",
      "\u001b[31mEpoch: 48, Loss: 0.5266553176426039\u001b[0m\n",
      "\u001b[31mEpoch: 49, Loss: 0.5262777524779129\u001b[0m\n",
      "\u001b[31mEpoch: 50, Loss: 0.5357357767296864\u001b[0m\n",
      "\u001b[31mEpoch: 51, Loss: 0.5256157385002585\u001b[0m\n",
      "\u001b[31mEpoch: 52, Loss: 0.5265684053781774\u001b[0m\n",
      "\u001b[31mEpoch: 53, Loss: 0.5161963112696949\u001b[0m\n",
      "\u001b[31mEpoch: 54, Loss: 0.5149484474103102\u001b[0m\n",
      "\u001b[31mEpoch: 55, Loss: 0.5159075511964073\u001b[0m\n",
      "\u001b[31mEpoch: 56, Loss: 0.515530872486169\u001b[0m\n",
      "\u001b[31mEpoch: 57, Loss: 0.5154330683100536\u001b[0m\n",
      "\u001b[31mEpoch: 58, Loss: 0.5154193877799903\u001b[0m\n",
      "\u001b[31mEpoch: 59, Loss: 0.4949742456985993\u001b[0m\n",
      "\u001b[31mEpoch: 60, Loss: 0.4859927316110456\u001b[0m\n",
      "\u001b[31mEpoch: 61, Loss: 0.4861361212967041\u001b[0m\n",
      "\u001b[31mEpoch: 62, Loss: 0.4757869889400202\u001b[0m\n",
      "\u001b[31mEpoch: 63, Loss: 0.4548063227076655\u001b[0m\n",
      "\u001b[31mEpoch: 64, Loss: 0.4557825099216418\u001b[0m\n",
      "\u001b[31mEpoch: 65, Loss: 0.4552425431875924\u001b[0m\n",
      "\u001b[31mEpoch: 66, Loss: 0.4560100919633323\u001b[0m\n",
      "\u001b[31mEpoch: 67, Loss: 0.4553708061073603\u001b[0m\n",
      "\u001b[31mEpoch: 68, Loss: 0.4556832393122076\u001b[0m\n",
      "\u001b[31mEpoch: 69, Loss: 0.4556739804031474\u001b[0m\n",
      "\u001b[31mEpoch: 70, Loss: 0.4562877884741579\u001b[0m\n",
      "\u001b[31mEpoch: 71, Loss: 0.4555540349627479\u001b[0m\n",
      "\u001b[31mEpoch: 72, Loss: 0.4548236563606432\u001b[0m\n",
      "\u001b[31mEpoch: 73, Loss: 0.4556973819345348\u001b[0m\n",
      "\u001b[31mEpoch: 74, Loss: 0.4553456460854758\u001b[0m\n",
      "\u001b[31mEpoch: 75, Loss: 0.454837625295147\u001b[0m\n",
      "\u001b[31mEpoch: 76, Loss: 0.4545826363820262\u001b[0m\n",
      "\u001b[31mEpoch: 77, Loss: 0.4554178521288244\u001b[0m\n",
      "\u001b[31mEpoch: 78, Loss: 0.454939654201604\u001b[0m\n",
      "\u001b[31mEpoch: 79, Loss: 0.4548362487543388\u001b[0m\n",
      "\u001b[31mEpoch: 80, Loss: 0.4553870824308654\u001b[0m\n",
      "\u001b[31mEpoch: 81, Loss: 0.4552928070813306\u001b[0m\n",
      "\u001b[31mEpoch: 82, Loss: 0.4544277578173729\u001b[0m\n",
      "\u001b[31mEpoch: 83, Loss: 0.4550998596132695\u001b[0m\n",
      "\u001b[31mEpoch: 84, Loss: 0.4547467331900802\u001b[0m\n",
      "\u001b[31mEpoch: 85, Loss: 0.4545182247928689\u001b[0m\n",
      "\u001b[31mEpoch: 86, Loss: 0.4551840119147569\u001b[0m\n",
      "\u001b[31mEpoch: 87, Loss: 0.4549625616953168\u001b[0m\n",
      "\u001b[31mEpoch: 88, Loss: 0.4550059691211034\u001b[0m\n",
      "\u001b[31mEpoch: 89, Loss: 0.4546462840997102\u001b[0m\n",
      "\u001b[31mEpoch: 90, Loss: 0.454372233579333\u001b[0m\n",
      "\u001b[31mEpoch: 91, Loss: 0.4557045220352067\u001b[0m\n",
      "\u001b[31mEpoch: 92, Loss: 0.4544644025241614\u001b[0m\n",
      "\u001b[31mEpoch: 93, Loss: 0.4554491777014866\u001b[0m\n",
      "\u001b[31mEpoch: 94, Loss: 0.4541618292362949\u001b[0m\n",
      "\u001b[31mEpoch: 95, Loss: 0.4556451954421926\u001b[0m\n",
      "\u001b[31mEpoch: 96, Loss: 0.4552143372567405\u001b[0m\n",
      "\u001b[31mEpoch: 97, Loss: 0.4552683292117235\u001b[0m\n",
      "\u001b[31mEpoch: 98, Loss: 0.455289846206649\u001b[0m\n",
      "\u001b[31mEpoch: 99, Loss: 0.4547838796809148\u001b[0m\n",
      "\n",
      "2019-06-28 22:55:42 Uploading - Uploading generated training model\n",
      "2019-06-28 22:55:42 Completed - Training job completed\n",
      "\u001b[31mEpoch: 100, Loss: 0.4545439449812143\u001b[0m\n",
      "\u001b[31m2019-06-28 22:55:31,820 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "Billable seconds: 457\n"
     ]
    }
   ],
   "source": [
    "# import a PyTorch wrapper\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# specify an output path\n",
    "output_path = f\"s3://{bucket}/{prefix}-5-hidden-layer\"\n",
    "\n",
    "# instantiate a pytorch estimator\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"pytorch\",\n",
    "    role=role,\n",
    "    framework_version=\"1.0\",\n",
    "    train_instance_count=1,\n",
    "    train_instance_type=\"ml.c4.xlarge\",\n",
    "    output_path=output_path,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    hyperparameters={\n",
    "        \"input_features\": 19,\n",
    "        \"hidden_dim\": 100, \n",
    "        \"output_dim\": 1,\n",
    "        \"epochs\": 100\n",
    "    })\n",
    "train_data_path = 's3://sagemaker-us-west-2-203336335427/starbucks-xgboost/train.csv'\n",
    "estimator.fit({'train': train_data_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "# Deploy the trained model\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "model = PyTorchModel(\n",
    "    entry_point=\"predict.py\",\n",
    "    role=role, \n",
    "    framework_version=\"1.0\",\n",
    "    model_data=estimator.model_data,\n",
    "    source_dir=\"pytorch\"\n",
    ")\n",
    "\n",
    "# deploy your model to create a predictor\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type=\"ml.t2.medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating the model\n",
    "# read in test data, assuming it is stored locally\n",
    "test_data = pd.read_csv(\"data/test_full.csv\", header=None, names=None)\n",
    "\n",
    "# labels are in the first column\n",
    "test_y = test_data.iloc[:,0]\n",
    "test_x = test_data.iloc[:,1:]\n",
    "test_y_preds = np.hstack(\n",
    "    predictor.predict(test_x.iloc[i: i+1000]).reshape(1, -1).squeeze() \n",
    "    for i in range(0, len(test_x), 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7605758026287857"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the test roc_auc_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(test_y, test_y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.770365320748121"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_y, test_y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-28 23:53:21 Starting - Starting the training job...\n",
      "2019-06-28 23:53:22 Starting - Launching requested ML instances...\n",
      "2019-06-28 23:54:19 Starting - Preparing the instances for training......\n",
      "2019-06-28 23:54:59 Downloading - Downloading input data..\n",
      "\n",
      "2019-06-28 23:55:34 Training - Training image download completed. Training in progress.\u001b[31mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[31mbash: no job control in this shell\u001b[0m\n",
      "\u001b[31m2019-06-28 23:55:35,877 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[31m2019-06-28 23:55:35,880 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[31m2019-06-28 23:55:35,892 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[31m2019-06-28 23:55:37,307 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[31m2019-06-28 23:55:37,575 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[31mGenerating setup.py\u001b[0m\n",
      "\u001b[31m2019-06-28 23:55:37,575 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[31m2019-06-28 23:55:37,575 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[31m2019-06-28 23:55:37,575 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[31m/usr/bin/python -m pip install -U . \u001b[0m\n",
      "\u001b[31mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[31mBuilding wheels for collected packages: train\n",
      "  Running setup.py bdist_wheel for train: started\n",
      "  Running setup.py bdist_wheel for train: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-pmpoqm1k/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[31mSuccessfully built train\u001b[0m\n",
      "\u001b[31mInstalling collected packages: train\u001b[0m\n",
      "\u001b[31mSuccessfully installed train-1.0.0\u001b[0m\n",
      "\u001b[31mYou are using pip version 18.1, however version 19.1.1 is available.\u001b[0m\n",
      "\u001b[31mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[31m2019-06-28 23:55:39,231 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[31m2019-06-28 23:55:39,243 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[31mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[31m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"hidden_dim\": 100,\n",
      "        \"input_features\": 19,\n",
      "        \"epochs\": 100,\n",
      "        \"output_dim\": 1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-pytorch-2019-06-28-23-53-20-934\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-203336335427/sagemaker-pytorch-2019-06-28-23-53-20-934/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[31m}\n",
      "\u001b[0m\n",
      "\u001b[31mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[31mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[31mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[31mSM_HPS={\"epochs\":100,\"hidden_dim\":100,\"input_features\":19,\"output_dim\":1}\u001b[0m\n",
      "\u001b[31mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[31mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[31mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[31mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[31mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[31mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[31mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[31mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[31mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[31mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[31mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[31mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[31mSM_MODULE_DIR=s3://sagemaker-us-west-2-203336335427/sagemaker-pytorch-2019-06-28-23-53-20-934/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[31mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":100,\"hidden_dim\":100,\"input_features\":19,\"output_dim\":1},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-pytorch-2019-06-28-23-53-20-934\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-203336335427/sagemaker-pytorch-2019-06-28-23-53-20-934/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[31mSM_USER_ARGS=[\"--epochs\",\"100\",\"--hidden_dim\",\"100\",\"--input_features\",\"19\",\"--output_dim\",\"1\"]\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[31mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[31mSM_HP_HIDDEN_DIM=100\u001b[0m\n",
      "\u001b[31mSM_HP_INPUT_FEATURES=19\u001b[0m\n",
      "\u001b[31mSM_HP_EPOCHS=100\u001b[0m\n",
      "\u001b[31mSM_HP_OUTPUT_DIM=1\u001b[0m\n",
      "\u001b[31mPYTHONPATH=/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[0m\n",
      "\u001b[31mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[31m/usr/bin/python -m train --epochs 100 --hidden_dim 100 --input_features 19 --output_dim 1\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[31mUsing device cpu.\u001b[0m\n",
      "\u001b[31mGet train data loader.\u001b[0m\n",
      "\u001b[31mEpoch: 1, Loss: 0.5889128709218922\u001b[0m\n",
      "\u001b[31mEpoch: 2, Loss: 0.5725185573184758\u001b[0m\n",
      "\u001b[31mEpoch: 3, Loss: 0.5639720688933291\u001b[0m\n",
      "\u001b[31mEpoch: 4, Loss: 0.5492341972842868\u001b[0m\n",
      "\u001b[31mEpoch: 5, Loss: 0.5474097650244219\u001b[0m\n",
      "\u001b[31mEpoch: 6, Loss: 0.546248509892252\u001b[0m\n",
      "\u001b[31mEpoch: 7, Loss: 0.5353469034030419\u001b[0m\n",
      "\u001b[31mEpoch: 8, Loss: 0.5349394480353884\u001b[0m\n",
      "\u001b[31mEpoch: 9, Loss: 0.5344005808842539\u001b[0m\n",
      "\u001b[31mEpoch: 10, Loss: 0.5340881635293309\u001b[0m\n",
      "\u001b[31mEpoch: 11, Loss: 0.5337987905606795\u001b[0m\n",
      "\u001b[31mEpoch: 12, Loss: 0.5335343646724126\u001b[0m\n",
      "\u001b[31mEpoch: 13, Loss: 0.5332719368428997\u001b[0m\n",
      "\u001b[31mEpoch: 14, Loss: 0.5329952299170726\u001b[0m\n",
      "\u001b[31mEpoch: 15, Loss: 0.5227902239075537\u001b[0m\n",
      "\u001b[31mEpoch: 16, Loss: 0.5125460145893168\u001b[0m\n",
      "\u001b[31mEpoch: 17, Loss: 0.5223978316158837\u001b[0m\n",
      "\u001b[31mEpoch: 18, Loss: 0.512308967271436\u001b[0m\n",
      "\u001b[31mEpoch: 19, Loss: 0.5020135519126158\u001b[0m\n",
      "\u001b[31mEpoch: 20, Loss: 0.5018352038907202\u001b[0m\n",
      "\u001b[31mEpoch: 21, Loss: 0.501652695749248\u001b[0m\n",
      "\u001b[31mEpoch: 22, Loss: 0.5015248161130184\u001b[0m\n",
      "\u001b[31mEpoch: 23, Loss: 0.5014164477865794\u001b[0m\n",
      "\u001b[31mEpoch: 24, Loss: 0.5013434675386113\u001b[0m\n",
      "\u001b[31mEpoch: 25, Loss: 0.5011858399831847\u001b[0m\n",
      "\u001b[31mEpoch: 26, Loss: 0.510605901340198\u001b[0m\n",
      "\u001b[31mEpoch: 27, Loss: 0.5010640052234412\u001b[0m\n",
      "\u001b[31mEpoch: 28, Loss: 0.4808973365224032\u001b[0m\n",
      "\u001b[31mEpoch: 29, Loss: 0.4707391775312942\u001b[0m\n",
      "\u001b[31mEpoch: 30, Loss: 0.4806314517406935\u001b[0m\n",
      "\u001b[31mEpoch: 31, Loss: 0.4604538155450357\u001b[0m\n",
      "\u001b[31mEpoch: 32, Loss: 0.4603758828598685\u001b[0m\n",
      "\u001b[31mEpoch: 33, Loss: 0.440256763856286\u001b[0m\n",
      "\u001b[31mEpoch: 34, Loss: 0.4301696334321624\u001b[0m\n",
      "\u001b[31mEpoch: 35, Loss: 0.4400151684110084\u001b[0m\n",
      "\u001b[31mEpoch: 36, Loss: 0.4399322617935777\u001b[0m\n",
      "\u001b[31mEpoch: 37, Loss: 0.4497848859942808\u001b[0m\n",
      "\u001b[31mEpoch: 38, Loss: 0.4496952357307802\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mEpoch: 39, Loss: 0.4496630337968301\u001b[0m\n",
      "\u001b[31mEpoch: 40, Loss: 0.4495197376368868\u001b[0m\n",
      "\u001b[31mEpoch: 41, Loss: 0.4495186710932505\u001b[0m\n",
      "\u001b[31mEpoch: 42, Loss: 0.4494383441235465\u001b[0m\n",
      "\u001b[31mEpoch: 43, Loss: 0.4493861407655455\u001b[0m\n",
      "\u001b[31mEpoch: 44, Loss: 0.4493260563368655\u001b[0m\n",
      "\u001b[31mEpoch: 45, Loss: 0.4491911900707622\u001b[0m\n",
      "\u001b[31mEpoch: 46, Loss: 0.4491100895801064\u001b[0m\n",
      "\u001b[31mEpoch: 47, Loss: 0.4490499835921807\u001b[0m\n",
      "\u001b[31mEpoch: 48, Loss: 0.4489972139147113\u001b[0m\n",
      "\u001b[31mEpoch: 49, Loss: 0.4490127199737543\u001b[0m\n",
      "\u001b[31mEpoch: 50, Loss: 0.4489405974606226\u001b[0m\n",
      "\u001b[31mEpoch: 51, Loss: 0.4488534836621767\u001b[0m\n",
      "\u001b[31mEpoch: 52, Loss: 0.44883547442292\u001b[0m\n",
      "\u001b[31mEpoch: 53, Loss: 0.448800710672408\u001b[0m\n",
      "\u001b[31mEpoch: 54, Loss: 0.4488281016101998\u001b[0m\n",
      "\u001b[31mEpoch: 55, Loss: 0.448681957400247\u001b[0m\n",
      "\u001b[31mEpoch: 56, Loss: 0.4486448284671102\u001b[0m\n",
      "\u001b[31mEpoch: 57, Loss: 0.448609420855282\u001b[0m\n",
      "\u001b[31mEpoch: 58, Loss: 0.4485049841137192\u001b[0m\n",
      "\u001b[31mEpoch: 59, Loss: 0.4485107427037388\u001b[0m\n",
      "\u001b[31mEpoch: 60, Loss: 0.4483438185677769\u001b[0m\n",
      "\u001b[31mEpoch: 61, Loss: 0.4483351715467173\u001b[0m\n",
      "\u001b[31mEpoch: 62, Loss: 0.4483300978464357\u001b[0m\n",
      "\u001b[31mEpoch: 63, Loss: 0.4482026749949777\u001b[0m\n",
      "\u001b[31mEpoch: 64, Loss: 0.4481748913557788\u001b[0m\n",
      "\u001b[31mEpoch: 65, Loss: 0.448129304439834\u001b[0m\n",
      "\u001b[31mEpoch: 66, Loss: 0.448110115151392\u001b[0m\n",
      "\u001b[31mEpoch: 67, Loss: 0.4480592865343398\u001b[0m\n",
      "\u001b[31mEpoch: 68, Loss: 0.4480462058234528\u001b[0m\n",
      "\u001b[31mEpoch: 69, Loss: 0.4479871011907689\u001b[0m\n",
      "\u001b[31mEpoch: 70, Loss: 0.4478587936735555\u001b[0m\n",
      "\u001b[31mEpoch: 71, Loss: 0.4479261983199959\u001b[0m\n",
      "\u001b[31mEpoch: 72, Loss: 0.4478163918780253\u001b[0m\n",
      "\u001b[31mEpoch: 73, Loss: 0.4478285776430302\u001b[0m\n",
      "\u001b[31mEpoch: 74, Loss: 0.447729799095611\u001b[0m\n",
      "\u001b[31mEpoch: 75, Loss: 0.447704207185614\u001b[0m\n",
      "\u001b[31mEpoch: 76, Loss: 0.4476915952892563\u001b[0m\n",
      "\u001b[31mEpoch: 77, Loss: 0.4476977283839652\u001b[0m\n",
      "\u001b[31mEpoch: 78, Loss: 0.4476217455296927\u001b[0m\n",
      "\u001b[31mEpoch: 79, Loss: 0.4475908948185292\u001b[0m\n",
      "\u001b[31mEpoch: 80, Loss: 0.4475508955087555\u001b[0m\n",
      "\u001b[31mEpoch: 81, Loss: 0.447591382521815\u001b[0m\n",
      "\u001b[31mEpoch: 82, Loss: 0.4475655308460698\u001b[0m\n",
      "\u001b[31mEpoch: 83, Loss: 0.4475016461720181\u001b[0m\n",
      "\u001b[31mEpoch: 84, Loss: 0.4475110089148252\u001b[0m\n",
      "\u001b[31mEpoch: 85, Loss: 0.4474622143565046\u001b[0m\n",
      "\u001b[31mEpoch: 86, Loss: 0.4474827154215133\u001b[0m\n",
      "\u001b[31mEpoch: 87, Loss: 0.4473940369034751\u001b[0m\n",
      "\u001b[31mEpoch: 88, Loss: 0.4473646231442132\u001b[0m\n",
      "\u001b[31mEpoch: 89, Loss: 0.4473572486125574\u001b[0m\n",
      "\u001b[31mEpoch: 90, Loss: 0.4473195040745011\u001b[0m\n",
      "\u001b[31mEpoch: 91, Loss: 0.4473055926768967\u001b[0m\n",
      "\u001b[31mEpoch: 92, Loss: 0.4472773637385404\u001b[0m\n",
      "\u001b[31mEpoch: 93, Loss: 0.445207672143362\u001b[0m\n",
      "\u001b[31mEpoch: 94, Loss: 0.4452505458173681\u001b[0m\n",
      "\u001b[31mEpoch: 95, Loss: 0.4452230673003733\u001b[0m\n",
      "\u001b[31mEpoch: 96, Loss: 0.4452607498451342\u001b[0m\n",
      "\u001b[31mEpoch: 97, Loss: 0.4451697313899404\u001b[0m\n",
      "\u001b[31mEpoch: 98, Loss: 0.4421678526660476\u001b[0m\n",
      "\u001b[31mEpoch: 99, Loss: 0.4421663154968608\u001b[0m\n",
      "\u001b[31mEpoch: 100, Loss: 0.4426909526759988\u001b[0m\n",
      "\u001b[31m2019-06-29 00:02:35,328 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2019-06-29 00:02:44 Uploading - Uploading generated training model\n",
      "2019-06-29 00:02:44 Completed - Training job completed\n",
      "Billable seconds: 465\n"
     ]
    }
   ],
   "source": [
    "# import a PyTorch wrapper\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# specify an output path\n",
    "output_path = f\"s3://{bucket}/{prefix}-5-hidden-layer\"\n",
    "\n",
    "# instantiate a pytorch estimator\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"pytorch\",\n",
    "    role=role,\n",
    "    framework_version=\"1.0\",\n",
    "    train_instance_count=1,\n",
    "    train_instance_type=\"ml.c4.xlarge\",\n",
    "    output_path=output_path,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    hyperparameters={\n",
    "        \"input_features\": 19,\n",
    "        \"hidden_dim\": 100, \n",
    "        \"output_dim\": 1,\n",
    "        \"epochs\": 100\n",
    "    })\n",
    "train_data_path = 's3://sagemaker-us-west-2-203336335427/starbucks-xgboost/train.csv'\n",
    "estimator.fit({'train': train_data_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "# Deploy the trained model\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "model = PyTorchModel(\n",
    "    entry_point=\"predict.py\",\n",
    "    role=role, \n",
    "    framework_version=\"1.0\",\n",
    "    model_data=estimator.model_data,\n",
    "    source_dir=\"pytorch\"\n",
    ")\n",
    "\n",
    "# deploy your model to create a predictor\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type=\"ml.t2.medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating the model\n",
    "# read in test data, assuming it is stored locally\n",
    "test_data = pd.read_csv(\"data/test_full.csv\", header=None, names=None)\n",
    "\n",
    "# labels are in the first column\n",
    "test_y = test_data.iloc[:,0]\n",
    "test_x = test_data.iloc[:,1:]\n",
    "test_y_preds = np.hstack(\n",
    "    predictor.predict(test_x.iloc[i: i+1000]).reshape(1, -1).squeeze() \n",
    "    for i in range(0, len(test_x), 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7843739514310259"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the test roc_auc_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(test_y, test_y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7980492920818039"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_y, test_y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up resources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove endpoint\n",
    "# predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
